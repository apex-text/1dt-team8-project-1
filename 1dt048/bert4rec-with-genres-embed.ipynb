{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6a34b44-dd0b-485a-8ce3-b76345ed336d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BERT4REC with genres embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59d5a199-b0c5-4925-a14c-fc7fb45ddea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### github 원본 출처\n",
    "* BERT4Rec\n",
    "* github 저장소 : constantfear/bert4rec\n",
    "* github link : https://github.com/constantfear/bert4rec?tab=readme-ov-file\n",
    "* paper link : https://arxiv.org/pdf/1904.06690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d81ed8-e95d-40fc-9726-1b82b9854f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9923cbef-a931-48f2-b7d8-f33b02438069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:18:36.462809Z",
     "iopub.status.busy": "2024-05-11T08:18:36.462436Z",
     "iopub.status.idle": "2024-05-11T08:18:39.885037Z",
     "shell.execute_reply": "2024-05-11T08:18:39.884089Z",
     "shell.execute_reply.started": "2024-05-11T08:18:36.462777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyspark.sql.functions import split, array_contains, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e775744-572b-4e08-8906-948fc83703c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Make configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d591b68-f245-4815-9611-2817c7eb0ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:18:42.937987Z",
     "iopub.status.busy": "2024-05-11T08:18:42.937473Z",
     "iopub.status.idle": "2024-05-11T08:18:42.944455Z",
     "shell.execute_reply": "2024-05-11T08:18:42.943304Z",
     "shell.execute_reply.started": "2024-05-11T08:18:42.937957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_len' : 80,\n",
    "    'hidden_units' : 256, # Embedding \n",
    "    'num_heads' : 2, # Multi-head layer \n",
    "    'num_layers': 2, # block encoder layer\n",
    "    'dropout_rate' : 0.1, # dropout\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 64,\n",
    "    'num_epochs' : 10,\n",
    "    'num_workers' : 0,\n",
    "    'mask_prob' : 0.15, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c72ce36-7b5a-4620-81a9-088b1b69134b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:18:43.119532Z",
     "iopub.status.busy": "2024-05-11T08:18:43.118602Z",
     "iopub.status.idle": "2024-05-11T08:18:43.172582Z",
     "shell.execute_reply": "2024-05-11T08:18:43.171499Z",
     "shell.execute_reply.started": "2024-05-11T08:18:43.119495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d313a4-bc54-4422-91fa-72201033bcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda768a4-d5c0-46e5-8b27-4fb0edadf24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Make Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f1246a-785b-4186-bc11-89834dc0a794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* train, validation 데이터 로드 및 통합\n",
    "* 사용자별 시청 영화 개수 필터링\n",
    "* 영화 장르 정보 처리\n",
    "* 사용자/아이템 인코더 및 디코더 생성\n",
    "* 사용자별 시청 시퀀스 데이터 생성 (학습/검증용 분리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba413b1d-1a1c-4af1-b7a6-ffbfdcfc02dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:18:43.282907Z",
     "iopub.status.busy": "2024-05-11T08:18:43.282463Z",
     "iopub.status.idle": "2024-05-11T08:18:43.299466Z",
     "shell.execute_reply": "2024-05-11T08:18:43.298580Z",
     "shell.execute_reply.started": "2024-05-11T08:18:43.282881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MakeSequenceDataSet():\n",
    "    \"\"\"\n",
    "    SequenceData\n",
    "    \n",
    "    사용자별로 시청한 영화 목록을 시퀀스로 정리한 테이블로 원본 테이블을 변환\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        print('Reading data...')\n",
    "        \n",
    "        # Spark로 CSV를 읽은 후 pandas DataFrame으로 변환\n",
    "        # rating이 4이상인 데이터만 사용\n",
    "        train_df = spark.table(\"`1dt_team8_databricks`.final.train_data\")[\n",
    "            ['userId', 'movieId', 'rating', 'timestamp']\n",
    "        ].filter(\"rating >= 4\").toPandas()\n",
    "        valid_df = spark.table(\"`1dt_team8_databricks`.final.validation_data\")[\n",
    "            ['userId', 'movieId', 'rating', 'timestamp']\n",
    "        ].filter(\"rating >= 4\").toPandas()\n",
    "\n",
    "        combined_df = pd.concat([train_df, valid_df], ignore_index=True)\n",
    "        self.df = combined_df\n",
    "\n",
    "        # userId 별로 카운트했을 때 movieID 개수가 4 이하인 userId를 제외\n",
    "        user_counts = self.df['userId'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= 4].index\n",
    "        self.df = self.df[self.df['userId'].isin(valid_users)].copy()\n",
    "        \n",
    "        self.movies = spark.table(\"1dt_team8_databricks.`movielens-32m`.movies\").toPandas()\n",
    "        \n",
    "        print('Applying genres...')\n",
    "        \n",
    "        self.genres = [\n",
    "            \"Action\",\n",
    "            \"Adventure\",\n",
    "            \"Animation\",\n",
    "            \"Children's\",\n",
    "            \"Comedy\",\n",
    "            \"Crime\",\n",
    "            \"Documentary\",\n",
    "            \"Drama\",\n",
    "            \"Fantasy\",\n",
    "            \"Film-Noir\",\n",
    "            \"Horror\",\n",
    "            \"Musical\",\n",
    "            \"Mystery\",\n",
    "            \"Romance\",\n",
    "            \"Sci-Fi\",\n",
    "            \"Thriller\",\n",
    "            \"War\",\n",
    "            \"Western\",\n",
    "        ]\n",
    "        \n",
    "        for genre in self.genres:\n",
    "            self.movies[genre] = self.movies[\"genres\"].apply(\n",
    "                lambda values: int(genre in values.split(\"|\"))\n",
    "            )\n",
    "        \n",
    "        self._movie_genres = self.movies[self.genres].to_numpy()\n",
    "        \n",
    "        print('Generate encoder and decoder...')\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder(self.movies['movieId'])\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder(self.df['userId'])\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df.loc[:, 'item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder.get(x, -1) + 1)\n",
    "        self.df.loc[:, 'user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder.get(x, -1))\n",
    "        \n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp']) \n",
    "        \n",
    "        print('Generate sequence data...')\n",
    "        self.user_train, self.genres_seq, self.user_valid = self.generate_sequence_data()\n",
    "        \n",
    "        print('Save tokenizer')\n",
    "        \n",
    "        # 변환기 저장 경로\n",
    "        tokenizer_path = '/Volumes/1dt_team8_databricks/bert_model/models'\n",
    "        os.makedirs(tokenizer_path, exist_ok=True)\n",
    "\n",
    "        # 변환기 저장\n",
    "        with open(os.path.join(tokenizer_path, 'bert4rec_model_0609_2_item_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.item_encoder, f)\n",
    "        with open(os.path.join(tokenizer_path, 'bert4rec_model_0609_2_item_decoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.item_decoder, f)\n",
    "        with open(os.path.join(tokenizer_path, 'bert4rec_model_0609_2_user_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.user_encoder, f)\n",
    "        with open(os.path.join(tokenizer_path, 'bert4rec_model_0609_2_user_decoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.user_decoder, f)\n",
    "\n",
    "        print('Finish!!!')\n",
    "\n",
    "    def generate_encoder_decoder(self, col) -> dict:\n",
    "        \"\"\"\n",
    "        encoder, decoder\n",
    "\n",
    "        Args:\n",
    "            col (str): columns\n",
    "        Returns:\n",
    "            dict: encoder, decoder\n",
    "        \"\"\"\n",
    "\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        ids = col.unique()\n",
    "\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def movie_genres(self, idx):\n",
    "        return self._movie_genres[idx-1].tolist()\n",
    "        \n",
    "    \n",
    "    def generate_sequence_data(self) -> dict:\n",
    "        \"\"\"\n",
    "        sequence_data\n",
    "\n",
    "        Returns:\n",
    "            dict: train user sequence / valid user sequence\n",
    "        \"\"\"\n",
    "        users = defaultdict(list)\n",
    "        user_train = {}\n",
    "        genres_seq = {}\n",
    "        user_valid = {}\n",
    "        group_df = self.df.groupby('user_idx')\n",
    "        for user, item in group_df:\n",
    "            users[user].extend(item['item_idx'].tolist())\n",
    "            \n",
    "        \n",
    "        for user in users:\n",
    "            user_train[user] = users[user][:-1]\n",
    "            genres_seq[user] = [self.movie_genres(i) for i in user_train[user]]\n",
    "            user_valid[user] = users[user][-1]\n",
    "\n",
    "        return user_train, genres_seq, user_valid\n",
    "    \n",
    "    def get_train_valid_data(self):\n",
    "        return self.user_train, self.genres_seq, self.user_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bbf6d3e-4ff2-4247-af19-17e24f6d26ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Make Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3b1e42-03f3-483b-81dc-6db812b554ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* test 데이터 로드\n",
    "* 사용자별 시청 영화 개수 필터링\n",
    "* 입력 시퀀스(Input)와 정답(Ground Truth) 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16d7dd0-8135-4748-b616-395ad53eeca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MakeTestSequenceDataSet():\n",
    "    \"\"\"\n",
    "    Test Sequence DataSet\n",
    "    \n",
    "    - test 테이블에서 데이터를 불러옵니다.\n",
    "    - 선호 영화가 13개 이상인 사용자만 필터링합니다.\n",
    "    - 각 사용자의 영화 목록에서 마지막 10개는 ground truth로,\n",
    "      나머지(최소 3개 이상)는 입력 시퀀스로 사용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print('Reading test data...')\n",
    "        self.df = spark.table(\"`1dt_team8_databricks`.final.test_data\")[\n",
    "            ['userId', 'movieId', 'rating', 'timestamp']\n",
    "        ].filter(\"rating >= 4\").toPandas()\n",
    "        user_counts = self.df['userId'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= 13].index\n",
    "        self.df = self.df[self.df['userId'].isin(valid_users)].copy()\n",
    "        \n",
    "        self.movies = spark.table(\"1dt_team8_databricks.`movielens-32m`.movies\").toPandas()\n",
    "        \n",
    "        print('Applying genres for test data...')\n",
    "        self.genres = [\n",
    "            \"Action\",\n",
    "            \"Adventure\",\n",
    "            \"Animation\",\n",
    "            \"Children's\",\n",
    "            \"Comedy\",\n",
    "            \"Crime\",\n",
    "            \"Documentary\",\n",
    "            \"Drama\",\n",
    "            \"Fantasy\",\n",
    "            \"Film-Noir\",\n",
    "            \"Horror\",\n",
    "            \"Musical\",\n",
    "            \"Mystery\",\n",
    "            \"Romance\",\n",
    "            \"Sci-Fi\",\n",
    "            \"Thriller\",\n",
    "            \"War\",\n",
    "            \"Western\",\n",
    "        ]\n",
    "        for genre in self.genres:\n",
    "            self.movies[genre] = self.movies[\"genres\"].apply(\n",
    "                lambda values: int(genre in values.split(\"|\"))\n",
    "            )\n",
    "        self._movie_genres = self.movies[self.genres].to_numpy()\n",
    "        \n",
    "        print('Generate encoder and decoder for test data...')\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder(self.movies['movieId'])\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder(self.df['userId'])\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "        \n",
    "        self.df = self.df.copy()\n",
    "        self.df['item_idx'] = self.df['movieId'].apply(lambda x: self.item_encoder.get(x, -1) + 1)\n",
    "        self.df['user_idx'] = self.df['userId'].apply(lambda x: self.user_encoder.get(x, -1))\n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp'])\n",
    "        \n",
    "        print('Generate test sequence data (split input & ground truth)...')\n",
    "        self.user_input, self.genres_input, self.user_groundtruth = self.generate_sequence_data()\n",
    "        print('Finish test data preparation!')\n",
    "    \n",
    "    def generate_encoder_decoder(self, col) -> dict:\n",
    "        encoder = {}\n",
    "        decoder = {}\n",
    "        for idx, _id in enumerate(col.unique()):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "        return encoder, decoder\n",
    "    \n",
    "    def movie_genres(self, idx):\n",
    "        return self._movie_genres[idx-1].tolist()\n",
    "    \n",
    "    def generate_sequence_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        각 사용자에 대해\n",
    "         - user_input: 영화 목록에서 마지막 10개를 제외한 나머지(입력 시퀀스)\n",
    "         - user_groundtruth: 마지막 10개 영화 (평가지표 측정용 정답)\n",
    "         - genres_input: 입력 영화에 대한 장르 시퀀스\n",
    "        \"\"\"\n",
    "        user_movies = {}\n",
    "        grouped = self.df.groupby('user_idx')\n",
    "        for user, items in grouped:\n",
    "            user_movies[user] = items['item_idx'].tolist()\n",
    "        \n",
    "        user_input = {}\n",
    "        genres_input = {}\n",
    "        user_groundtruth = {}\n",
    "        for user, seq in user_movies.items():\n",
    "            # 이미 13개 이상인 사용자만 있으므로\n",
    "            user_input[user] = seq[:-10]\n",
    "            user_groundtruth[user] = seq[-10:]\n",
    "            genres_input[user] = [self.movie_genres(i) for i in user_input[user]]\n",
    "        return user_input, genres_input, user_groundtruth\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        return self.user_input, self.genres_input, self.user_groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c3a017-c211-43f6-8329-7d6590e17b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3. Define BERT Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c96013-6407-48a8-ae8b-7b00bbb3c21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Masking, Random Replacement 등 BERT 학습을 위한 토큰 변환 로직 구현<br>\n",
    "이제 학습을 위한 Dataset 클래스를 설명하겠습니다. 해당 데이터셋은 **사용자들과 그들의 시청 순서(시퀀스)**로 구성됩니다. 모델 학습 시, 입력되는 시퀀스는 다음과 같은 방식으로 수정됩니다:\n",
    "\n",
    "* 시퀀스에서 무작위로 15%의 토큰을 선택합니다.\n",
    "* 선택된 토큰들에 대해 다음과 같이 처리합니다:\n",
    "    * 80%는 마스크 토큰으로 대체됩니다.\n",
    "    * 10%는 무작위 토큰으로 대체됩니다.\n",
    "    * 나머지 10%는 변경되지 않습니다.\n",
    "\n",
    "시청한 영화 시퀀스 외에도, 각 영화에 해당하는 장르 시퀀스가 있으며, 이 역시 영화 시퀀스와 같은 방식으로 수정됩니다.<br>\n",
    "따라서, 학습 데이터셋의 하나의 요소는 다음과 같은 구조를 가집니다:\n",
    "\n",
    "Dataset[i] = item_sequence, genres_sequence, labels\n",
    "\n",
    "* item_sequence — 변경된 영화 시퀀스 [1 x n] (여기서 n은 시퀀스 길이)\n",
    "* genres_sequence — 영화에 해당하는 장르의 변경된 시퀀스 [m x n] (m은 장르 수, 이 경우 m = 18)\n",
    "* labels — 실제 시청한 영화들의 원래 시퀀스로, 모델이 예측해야 할 정답 [1 x n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d80e1ce-e479-4481-87f9-2624a6ce3cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:22:08.411096Z",
     "iopub.status.busy": "2024-05-11T08:22:08.410627Z",
     "iopub.status.idle": "2024-05-11T08:22:08.425672Z",
     "shell.execute_reply": "2024-05-11T08:22:08.424711Z",
     "shell.execute_reply.started": "2024-05-11T08:22:08.411053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTRecDataSet(Dataset):\n",
    "    #def __init__(self, user_train, movie_genres, max_len, num_user, num_item, mask_prob):\n",
    "    def __init__(self, sequence_dataset, user_train, genres_seq, max_len, num_user, num_item, mask_prob):\n",
    "        self.sequence_dataset = sequence_dataset # MakeSequenceDataSet 객체를 저장\n",
    "        self.user_train = user_train\n",
    "        #self.movie_genres = movie_genres\n",
    "        self.genres_seq = genres_seq\n",
    "        self.max_len = max_len\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "        self._all_items = set([i for i in range(1, self.num_item + 1)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_user\n",
    "\n",
    "    def __getitem__(self, user): \n",
    "        \n",
    "        user_seq = self.user_train[user]\n",
    "        genre_seq = self.genres_seq[user]\n",
    "        tokens = []\n",
    "        genres_seq = []\n",
    "        labels = []\n",
    "        \n",
    "        for s, g in zip(user_seq[-self.max_len:], genre_seq[-self.max_len:]):\n",
    "            prob = np.random.random()\n",
    "            if prob < self.mask_prob:\n",
    "                prob /= self.mask_prob\n",
    "                if prob < 0.8:\n",
    "                    # masking\n",
    "                    tokens.append(self.num_item + 1)\n",
    "                    genres_seq.append([1]*18)\n",
    "                elif prob < 0.9:\n",
    "                    # noise\n",
    "                    rnd_token = self.random_neg_sampling(rated_item = user_seq, num_item_sample = 1)[0]\n",
    "                    tokens.append(rnd_token)\n",
    "                    genres_seq.append(self.sequence_dataset.movie_genres(rnd_token))\n",
    "                else:\n",
    "                    tokens.append(s)\n",
    "                    genres_seq.append(g)\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "                genres_seq.append(g)\n",
    "            labels.append(s)\n",
    "\n",
    "        mask_len = self.max_len - len(tokens)\n",
    "        \n",
    "        tokens = [0] * mask_len + tokens\n",
    "        genres_seq = [[0]*18] * mask_len + genres_seq\n",
    "        labels = [0] * mask_len + labels\n",
    "\n",
    "        return torch.LongTensor(tokens), torch.Tensor(genres_seq), torch.LongTensor(labels)\n",
    "\n",
    "    def random_neg_sampling(self, rated_item: list, num_item_sample: int):\n",
    "        \"\"\"\n",
    "        사용자가 시청한 아이템을 제외한 나머지 아이템 중에서\n",
    "        num_item_sample 개수만큼 랜덤으로 샘플링합니다.\n",
    "        \"\"\"\n",
    "        candidates = list(self._all_items - set(rated_item))\n",
    "        \n",
    "        # 만약 남은 후보가 뽑으려는 샘플 수보다 적으면, 남은 만큼만 뽑도록 처리\n",
    "        if len(candidates) < num_item_sample:\n",
    "            return random.sample(candidates, len(candidates))\n",
    "            \n",
    "        return random.sample(candidates, num_item_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e001a1d9-2f31-4d89-9426-133b93c18bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:25:24.544364Z",
     "iopub.status.busy": "2024-05-11T08:25:24.543714Z",
     "iopub.status.idle": "2024-05-11T08:25:24.890291Z",
     "shell.execute_reply": "2024-05-11T08:25:24.889278Z",
     "shell.execute_reply.started": "2024-05-11T08:25:24.544331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 기존 train 데이터 (MakeSequenceDataSet) 그대로 사용\n",
    "make_sequence_dataset = MakeSequenceDataSet()\n",
    "user_train, movie_genres, user_valid = make_sequence_dataset.get_train_valid_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301238e5-9759-4cd3-be21-b90f871c9826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# test 데이터는 별도 테이블에서 불러옴 (MakeTestSequenceDataSet)\n",
    "make_test_dataset = MakeTestSequenceDataSet()\n",
    "user_input, genres_input, user_groundtruth = make_test_dataset.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdfb336f-6962-4dc0-8a94-6b586eb48785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4. Create Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947740c9-0ce0-4322-a4cd-8b1e8c165bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 학습을 위한 배치(Batch) 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78e0eb4-d43c-4ee0-8d13-518498cef6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:25:24.892116Z",
     "iopub.status.busy": "2024-05-11T08:25:24.891680Z",
     "iopub.status.idle": "2024-05-11T08:25:24.909117Z",
     "shell.execute_reply": "2024-05-11T08:25:24.908211Z",
     "shell.execute_reply.started": "2024-05-11T08:25:24.892085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bert4rec_dataset = BERTRecDataSet(\n",
    "    sequence_dataset = make_sequence_dataset, # make_sequence_dataset 객체 전달\n",
    "    user_train = user_train,\n",
    "    genres_seq = movie_genres, # movie_genres = movie_genres,\n",
    "    max_len = config['max_len'], \n",
    "    num_user = make_sequence_dataset.num_user, \n",
    "    num_item = make_sequence_dataset.num_item,\n",
    "    mask_prob = config['mask_prob'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061bead4-60b4-4183-abf3-9e924827c26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:25:24.911529Z",
     "iopub.status.busy": "2024-05-11T08:25:24.911191Z",
     "iopub.status.idle": "2024-05-11T08:25:24.917669Z",
     "shell.execute_reply": "2024-05-11T08:25:24.916626Z",
     "shell.execute_reply.started": "2024-05-11T08:25:24.911503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    bert4rec_dataset, \n",
    "    batch_size = config['batch_size'], \n",
    "    shuffle = True, \n",
    "    pin_memory = True,\n",
    "    num_workers = config['num_workers'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dfddd5f-087a-4537-a000-10354680df47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019f90a8-f39c-4ec1-8c23-94e9c880a3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:25:24.919201Z",
     "iopub.status.busy": "2024-05-11T08:25:24.918867Z",
     "iopub.status.idle": "2024-05-11T08:25:26.290361Z",
     "shell.execute_reply": "2024-05-11T08:25:26.289253Z",
     "shell.execute_reply.started": "2024-05-11T08:25:24.919176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "d = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac98fd8-9e77-4446-8bc2-5f87683b29f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7cb6da-9776-4f63-af20-7e0dbc93ee8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b5dcee-e222-4f21-966e-7ba1d41d9894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.1. Define Model Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2619b244-4d01-404a-a8ae-360b2bef30d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 임베딩 (Embedding)\n",
    "  * TokenEmbedding (영화 아이템 임베딩)\n",
    "  * PositionalEmbedding (위치 임베딩)\n",
    "  * GenresEmbedding (장르 임베딩)\n",
    "  * BERTEmbedding (위 세 가지 임베딩 결합)\n",
    "<br>\n",
    "<br>\n",
    "* 트랜스포머 블록 (Transformer Block)\n",
    "  * Attention 및 MultiHeadedAttention\n",
    "  * PositionwiseFeedForward\n",
    "  * SublayerConnection, LayerNorm 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b070c68-7ccd-4653-8103-28d3ca01c224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:18.084252Z",
     "iopub.status.busy": "2024-05-11T08:26:18.083435Z",
     "iopub.status.idle": "2024-05-11T08:26:18.094712Z",
     "shell.execute_reply": "2024-05-11T08:26:18.093598Z",
     "shell.execute_reply.started": "2024-05-11T08:26:18.084211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "        \n",
    "class GenresEmbedding(nn.Module):\n",
    "    def __init__(self, genres_size, embed_size=512):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(genres_size, genres_size*2)\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(genres_size*2, embed_size)\n",
    "    def forward(self, genres_vec):\n",
    "        x = self.linear_2(self.act(self.linear_1(genres_vec)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a218f5-9fc6-4e9d-8779-341e487778ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:18.357598Z",
     "iopub.status.busy": "2024-05-11T08:26:18.357260Z",
     "iopub.status.idle": "2024-05-11T08:26:18.365335Z",
     "shell.execute_reply": "2024-05-11T08:26:18.364395Z",
     "shell.execute_reply.started": "2024-05-11T08:26:18.357558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, genres_size, embed_size, max_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.genres_emb = GenresEmbedding(genres_size=genres_size, embed_size=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, genres):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.genres_emb(genres)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f2d3b3-a5f0-45a6-833f-1c80e1a4265f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:18.553037Z",
     "iopub.status.busy": "2024-05-11T08:26:18.552745Z",
     "iopub.status.idle": "2024-05-11T08:26:18.559780Z",
     "shell.execute_reply": "2024-05-11T08:26:18.558848Z",
     "shell.execute_reply.started": "2024-05-11T08:26:18.553012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de022bf7-111b-46f9-8038-0c2bfd6c65c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:19.437024Z",
     "iopub.status.busy": "2024-05-11T08:26:19.436695Z",
     "iopub.status.idle": "2024-05-11T08:26:19.447145Z",
     "shell.execute_reply": "2024-05-11T08:26:19.446070Z",
     "shell.execute_reply.started": "2024-05-11T08:26:19.437001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10834aaf-89fc-4d78-9009-b38f23451069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:19.788373Z",
     "iopub.status.busy": "2024-05-11T08:26:19.787545Z",
     "iopub.status.idle": "2024-05-11T08:26:19.793843Z",
     "shell.execute_reply": "2024-05-11T08:26:19.792913Z",
     "shell.execute_reply.started": "2024-05-11T08:26:19.788344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2985271-9218-45d9-884c-e8314fcb79c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:20.019320Z",
     "iopub.status.busy": "2024-05-11T08:26:20.018523Z",
     "iopub.status.idle": "2024-05-11T08:26:20.026243Z",
     "shell.execute_reply": "2024-05-11T08:26:20.024334Z",
     "shell.execute_reply.started": "2024-05-11T08:26:20.019293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42aafb18-1860-4098-8ca1-c14e4b4617c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:20.228265Z",
     "iopub.status.busy": "2024-05-11T08:26:20.227425Z",
     "iopub.status.idle": "2024-05-11T08:26:20.235193Z",
     "shell.execute_reply": "2024-05-11T08:26:20.234293Z",
     "shell.execute_reply.started": "2024-05-11T08:26:20.228231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09b9992-5d4c-494d-a44d-ab4d4229d63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:20.543000Z",
     "iopub.status.busy": "2024-05-11T08:26:20.542684Z",
     "iopub.status.idle": "2024-05-11T08:26:20.549020Z",
     "shell.execute_reply": "2024-05-11T08:26:20.548094Z",
     "shell.execute_reply.started": "2024-05-11T08:26:20.542975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b008a6-c51f-4430-94c2-704c85d5c3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:20.985308Z",
     "iopub.status.busy": "2024-05-11T08:26:20.984977Z",
     "iopub.status.idle": "2024-05-11T08:26:20.993514Z",
     "shell.execute_reply": "2024-05-11T08:26:20.992503Z",
     "shell.execute_reply.started": "2024-05-11T08:26:20.985281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc20c382-b813-4eaf-83c0-028d4823654b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.2. Assembling BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2865dbff-a43e-4dfb-a2bc-858b1b5358d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:21.534063Z",
     "iopub.status.busy": "2024-05-11T08:26:21.533715Z",
     "iopub.status.idle": "2024-05-11T08:26:21.544156Z",
     "shell.execute_reply": "2024-05-11T08:26:21.543140Z",
     "shell.execute_reply.started": "2024-05-11T08:26:21.534033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_max_len, num_items, genres_size, bert_num_blocks, bert_num_heads,\n",
    "                 bert_hidden_units, bert_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # fix_random_seed_as(args.model_init_seed)\n",
    "        # self.init_weights()\n",
    "\n",
    "        max_len = bert_max_len\n",
    "        num_items = num_items\n",
    "        n_layers = bert_num_blocks\n",
    "        heads = bert_num_heads\n",
    "        self.vocab_size = num_items + 2\n",
    "        self.genres_size = genres_size\n",
    "        hidden = bert_hidden_units\n",
    "        self.hidden = hidden\n",
    "        dropout = bert_dropout\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=self.vocab_size, genres_size=self.genres_size, \n",
    "                                       embed_size=self.hidden, max_len=max_len, dropout=dropout)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "        self.out = nn.Linear(hidden, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, genres):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "#        print(x.shape)\n",
    "        x = self.embedding(x, genres)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd17d555-0929-40ef-a585-202881b23c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "모델의 작동 원리:\n",
    "\n",
    "1) 모델의 입력으로는 사용자가 시청한 영화들의 시퀀스와, 해당 영화들의 장르 시퀀스가 주어집니다. 모델의 출력은 시청한 영화들에 대한 로짓(logits) 시퀀스입니다.\n",
    "\n",
    "2) 모델 학습 시, 전체 시퀀스를 예측하도록 모델을 학습시킵니다. 즉, 모델이 마스킹된 토큰들을 구별하고, 그 자리에 올바른 토큰을 출력할 수 있도록 학습합니다.\n",
    "\n",
    "3) 모델을 실제 사용할 때는, 사용자의 시청 영화 시퀀스 끝에 마스크 토큰을 추가하고, 모델은 전체 시퀀스를 출력합니다. 이 출력 시퀀스의 마지막 요소가 모델이 추천하는 다음 영화가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b8c134-4de8-47fd-86a0-dbf2461c3914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:25.648534Z",
     "iopub.status.busy": "2024-05-11T08:26:25.648192Z",
     "iopub.status.idle": "2024-05-11T08:26:25.837293Z",
     "shell.execute_reply": "2024-05-11T08:26:25.836442Z",
     "shell.execute_reply.started": "2024-05-11T08:26:25.648505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = BERT(\n",
    "    num_items = make_sequence_dataset.num_item,\n",
    "    genres_size = 18,\n",
    "    bert_hidden_units = config['hidden_units'], \n",
    "    bert_num_heads = config['num_heads'], \n",
    "    bert_num_blocks = config['num_layers'], \n",
    "    bert_max_len = config['max_len'], \n",
    "    bert_dropout = config['dropout_rate'], \n",
    ").to(device)\n",
    "metrics = {'train_loss': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9572b989-c7d1-4c06-8a5b-625b911de182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 모델 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf921d8-5232-40f3-b34e-317b26cdab7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0412231f-d9fb-4915-8007-babe4355ac6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 출력 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049986e4-dc1e-4814-b86e-bf425eb508c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T11:03:42.530168Z",
     "iopub.status.busy": "2024-05-11T11:03:42.529308Z",
     "iopub.status.idle": "2024-05-11T11:03:42.698018Z",
     "shell.execute_reply": "2024-05-11T11:03:42.697120Z",
     "shell.execute_reply.started": "2024-05-11T11:03:42.530136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq, genres, _ = d\n",
    "output = model(seq.to(device), genres.to(device))\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76aa4056-1411-4358-aa77-9ec8cf45b7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Training and Evaluaton Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c7af82-8cc6-48f8-abc8-9c6560b4879c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:22.605064Z",
     "iopub.status.busy": "2024-05-11T08:26:22.604681Z",
     "iopub.status.idle": "2024-05-11T08:26:22.609732Z",
     "shell.execute_reply": "2024-05-11T08:26:22.608611Z",
     "shell.execute_reply.started": "2024-05-11T08:26:22.605035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d675bb-5cd7-494f-93de-f5c68ddc9609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T09:54:13.614706Z",
     "iopub.status.busy": "2024-05-11T09:54:13.613886Z",
     "iopub.status.idle": "2024-05-11T09:54:13.620894Z",
     "shell.execute_reply": "2024-05-11T09:54:13.619903Z",
     "shell.execute_reply.started": "2024-05-11T09:54:13.614671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e878e7-ada7-4902-87ba-5c59f86def17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1. Define Functions for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2021f17f-9b97-4096-b199-16f4a3202daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T09:28:10.736286Z",
     "iopub.status.busy": "2024-05-11T09:28:10.735435Z",
     "iopub.status.idle": "2024-05-11T09:28:10.752555Z",
     "shell.execute_reply": "2024-05-11T09:28:10.751617Z",
     "shell.execute_reply.started": "2024-05-11T09:28:10.736252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    i = len(metrics['train_loss'])\n",
    "    #, 'dev_ndcg': [] }\n",
    "    for seq, genres, labels in data_loader:\n",
    "        seq, genres, labels = seq.to(device), genres.to(device), labels.to(device)\n",
    "        logits = model(seq, genres) # (bs, t, vocab)\n",
    "        logits = logits.view(-1, logits.size(-1)) # (bs * t, vocab)\n",
    "        labels = labels.view(-1) # (bs * t)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss_val += loss.item()\n",
    "        metrics['train_loss'].append((i, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "       \n",
    "        if i%100 == 0:\n",
    "#             metrics['dev_ndcg'].append((i, compute_ndcg(model, dev_inp, dev_out)))\n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(5,4))\n",
    "            for j, (name, history) in enumerate(sorted(metrics.items())):\n",
    "#                 plt.subplot(1, len(metrics), j + 1)\n",
    "                plt.title(name)\n",
    "                plt.plot(*zip(*history))\n",
    "                plt.grid()\n",
    "            plt.show()\n",
    "        i += 1\n",
    "    \n",
    "    loss_val /= len(data_loader)\n",
    "\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb107bc-257b-4a05-ba1e-df2ca2eb2bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model, user_input, user_groundtruth, max_len, bert4rec_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    test 데이터 평가 함수\n",
    "      - user_input: 각 사용자의 입력 시퀀스 (영화 id 리스트)\n",
    "      - user_groundtruth: 각 사용자의 ground truth 영화 (마지막 10개)\n",
    "      - test_dataset: test dataset 클래스 (장르 정보 제공용)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    NDCG = 0.0\n",
    "    HIT = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    num_item_sample = 100\n",
    "    users = list(user_input.keys())\n",
    "    \n",
    "    for user in tqdm(users):\n",
    "        input_seq = user_input[user]\n",
    "        groundtruth = set(user_groundtruth[user])\n",
    "        \n",
    "        # 입력 시퀀스를 max_len에 맞게 처리 (패딩)\n",
    "        seq = input_seq[-max_len:]\n",
    "        genre_seq = [test_dataset.movie_genres(i) for i in input_seq][-max_len:]\n",
    "        padding_len = max_len - len(seq)\n",
    "        seq = [0]*padding_len + seq\n",
    "        genre_seq = [[0]*18 for _ in range(padding_len)] + genre_seq\n",
    "        \n",
    "        rated_items = set(input_seq) | groundtruth\n",
    "        neg_items = bert4rec_dataset.random_neg_sampling(rated_item=list(rated_items), num_item_sample=num_item_sample)\n",
    "        items = list(groundtruth) + neg_items  # 후보군 구성\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            seq_tensor = torch.LongTensor([seq]).to(device)\n",
    "            genre_tensor = torch.FloatTensor([genre_seq]).to(device)\n",
    "            logits = model(seq_tensor, genre_tensor)\n",
    "            scores = -logits[0, -1, items]  # 마지막 토큰의 출력 점수 활용\n",
    "            \n",
    "            top_k = torch.topk(scores, k=10, largest=False).indices\n",
    "            top_k_items = set([items[i] for i in top_k.cpu().numpy()])\n",
    "            \n",
    "            hit_items = top_k_items & groundtruth\n",
    "            precision += len(hit_items) / 10.0\n",
    "            recall += len(hit_items) / len(groundtruth)\n",
    "            HIT += 1 if len(hit_items) > 0 else 0\n",
    "            \n",
    "            dcg = 0.0\n",
    "            for rank, idx in enumerate(top_k.cpu().numpy()):\n",
    "                item = items[idx]\n",
    "                if item in groundtruth:\n",
    "                    dcg += 1 / np.log2(rank + 2)\n",
    "            ideal_dcg = sum([1 / np.log2(i + 2) for i in range(10)])\n",
    "            NDCG += dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "    total_users = len(users)\n",
    "    ndcg = NDCG / total_users\n",
    "    hit = HIT / total_users\n",
    "    prec = precision / total_users\n",
    "    rec = recall / total_users\n",
    "\n",
    "    print(f'[Test] NDCG@10: {ndcg:.4f} | HIT@10: {hit:.4f}')\n",
    "    print(f'[Test] Precision@10: {prec:.4f} | Recall@10: {rec:.4f}')\n",
    "    \n",
    "    return ndcg, hit, prec, rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b7045bc-eb6e-421b-adbb-f7eca9ea39a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 손실 함수와 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaddb932-2e12-44b4-9e50-ddf7ee30742a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:27.052306Z",
     "iopub.status.busy": "2024-05-11T08:26:27.051958Z",
     "iopub.status.idle": "2024-05-11T08:26:27.058127Z",
     "shell.execute_reply": "2024-05-11T08:26:27.056987Z",
     "shell.execute_reply.started": "2024-05-11T08:26:27.052280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0180a022-d258-4f90-afd2-0f2564a68249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78014b00-bdbe-42dc-9c86-431b1deb02bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T08:26:29.005238Z",
     "iopub.status.busy": "2024-05-11T08:26:29.004363Z",
     "iopub.status.idle": "2024-05-11T09:04:59.477192Z",
     "shell.execute_reply": "2024-05-11T09:04:59.475946Z",
     "shell.execute_reply.started": "2024-05-11T08:26:29.005189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "loss_list = []\n",
    "for epoch in tqdm(range(1, config['num_epochs'] + 1)):\n",
    "    train_loss = train(model, criterion, optimizer, data_loader)\n",
    "    loss_list.append(train_loss)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f'Epoch: {epoch:3d} | Train loss: {train_loss:.5f}')\n",
    "    \n",
    "    # 메모리 정리\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74a7c0c-6d87-4ff8-ae2c-d8002303decc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T09:04:59.613679Z",
     "iopub.status.busy": "2024-05-11T09:04:59.613396Z",
     "iopub.status.idle": "2024-05-11T09:04:59.872520Z",
     "shell.execute_reply": "2024-05-11T09:04:59.871608Z",
     "shell.execute_reply.started": "2024-05-11T09:04:59.613655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c49718-02cd-48ad-b89c-cf6bd1169e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.3. Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec640379-a065-4842-ac2e-2e108c2f9da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T09:28:14.194872Z",
     "iopub.status.busy": "2024-05-11T09:28:14.193797Z",
     "iopub.status.idle": "2024-05-11T09:38:26.123889Z",
     "shell.execute_reply": "2024-05-11T09:38:26.122995Z",
     "shell.execute_reply.started": "2024-05-11T09:28:14.194836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ndcg, hit, precision, recall = evaluate_test(\n",
    "    model=model,\n",
    "    user_input=user_input,\n",
    "    user_groundtruth=user_groundtruth,\n",
    "    max_len=config['max_len'],\n",
    "    bert4rec_dataset=bert4rec_dataset,\n",
    "    test_dataset=make_test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93cc878b-c383-4f5a-ad47-a813530b4dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 평가지표 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c657c7-7665-4af9-9ff6-e4546d2db7af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2024-05-11T10:18:10.539774Z",
     "iopub.status.busy": "2024-05-11T10:18:10.539073Z",
     "iopub.status.idle": "2024-05-11T10:18:10.544547Z",
     "shell.execute_reply": "2024-05-11T10:18:10.543593Z",
     "shell.execute_reply.started": "2024-05-11T10:18:10.539743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'NDCG@10: {ndcg}| HIT@10: {hit}')\n",
    "print(f'precision@10: {precision}| recall@10: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0307ecf8-e97b-49a5-8ec2-1eab4e4e6a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083ea240-5f92-4468-a88d-5dac7e3fb531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "torch.save(model.state_dict(), \"/Volumes/1dt_team8_databricks/bert_model/models/bert4rec_model_0609_2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bert4rec-with-genres-embed",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 339,
     "sourceId": 77759,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
