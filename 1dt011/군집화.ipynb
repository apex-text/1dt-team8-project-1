{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa73ceb8-b146-4191-929b-52fc3158a08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "┌──────────────────────┐\n",
    "│ 1. 데이터 로딩        │\n",
    "│ - movies.csv         │\n",
    "│ - ratings.csv        │\n",
    "└─────────┬────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌──────────────────────────────────────┐\n",
    "│ 2. 장르 전처리                        │\n",
    "│ - 장르 explode 및 원-핫 인코딩         │\n",
    "└─────────┬────────────────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌──────────────────────────────────────┐\n",
    "│ 3. 사용자 장르 선호도 벡터화           │\n",
    "│ - 장르 x 평점 → 사용자 프로파일 생성    │\n",
    "└─────────┬────────────────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌────────────────────────────┐\n",
    "│ 4. 사용자 벡터 → features   │\n",
    "│ - VectorAssembler 사용     │\n",
    "└─────────┬──────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌────────────────────────────┐\n",
    "│ 5. KMeans 군집화            │\n",
    "│ - 사용자 군집 분류           │\n",
    "└─────────┬──────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌────────────────────────────────────┐\n",
    "│ 6. 클러스터 내 영화 평균 평점 계산    │\n",
    "│ - 각 군집 내 인기 영화 파악          │\n",
    "└─────────┬──────────────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌─────────────────────────────────────┐\n",
    "│ 7. 특정 사용자에게 추천 수행          │\n",
    "│ - 같은 클러스터 + 안 본 영화          │\n",
    "│ - 평균 평점 기반 상위 N개 추천        │\n",
    "└─────────┬───────────────────────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌─────────────────────────────────────┐\n",
    "│ 8. 추천 결과 출력                    │\n",
    "│ - movieId, title, genres, avg_rating│\n",
    "└─────────────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b40eee-77ea-40ae-b643-e616a9a76957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_movies_by_user(userId: int):\n",
    "    catalog = \"1dt_team8_databricks\"\n",
    "    schema = \"final\"\n",
    "    path = f\"{catalog}.{schema}\"\n",
    "    \n",
    "    df1 = spark.read.table(f\"{path}.movies\")\n",
    "    df2 = spark.read.table(f\"{path}.links\")      \n",
    "    df3 = spark.read.table(f\"{path}.ratings\")\n",
    "    df4 = spark.read.table(f\"{path}.tags\")   \n",
    "\n",
    "    movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    feature_cols = [c for c in user_profile.columns if c.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "\n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\") \\\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "    user_seen_movies = df3.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    user_cluster = user_clusters.filter(col(\"userId\") == userId).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "\n",
    "    indexed = top_recommendations.withColumn(\n",
    "        \"index\", row_number().over(Window.orderBy(col(\"movieId\"))) - 1\n",
    "    ).select(\"index\", \"movieId\")\n",
    "\n",
    "    display(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fe2bca-f68e-4a13-ba67-80bc71226049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recommend_movies_by_user(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a8bb5c-d526-438d-ad95-40294f95460c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##ML Flow 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc979f30-f26d-4066-b412-2a798a378eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def run_kmeans_with_mlflow(user_profile_df, k=5, seed=42):\n",
    "    # ✅ MLflow 실험 위치 설정\n",
    "    mlflow.set_experiment(\"/Users/1dt011@msacademy.msai.kr/1dt011\")\n",
    "\n",
    "    feature_cols = [col for col in user_profile_df.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile_df)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"KMeans_User_Clustering\"):\n",
    "        mlflow.log_param(\"k\", k)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "\n",
    "        kmeans = KMeans(k=k, seed=seed)\n",
    "        model = kmeans.fit(user_features)\n",
    "\n",
    "        # 클러스터링 비용(Within Set Sum of Squared Errors)\n",
    "        mlflow.log_metric(\"wsse\", model.summary.trainingCost)\n",
    "\n",
    "        # 모델 저장\n",
    "        mlflow.spark.log_model(model, \"kmeans_model\")\n",
    "\n",
    "        # userId와 클러스터 예측 결과만 반환\n",
    "        return model.transform(user_features).select(\"userId\", \"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fff4da1-8d5c-40e7-b02d-a04054272fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "movies_df = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "ratings_df = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "\n",
    "# 2. 타입 변환\n",
    "ratings_df = ratings_df.withColumn(\"movieId\", col(\"movieId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\")) \\\n",
    "                       .withColumn(\"userId\", col(\"userId\").cast(\"int\"))\n",
    "movies_df = movies_df.withColumn(\"movieId\", col(\"movieId\").cast(\"int\"))\n",
    "\n",
    "# 3. 장르 explode\n",
    "movie_genres = movies_df.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))).select(\"movieId\", \"genre\")\n",
    "\n",
    "# 4. 평점과 장르 조인\n",
    "ratings_with_genre = ratings_df.join(movie_genres, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# 5. 사용자별 장르별 평균 평점 계산\n",
    "user_genre_pref = ratings_with_genre.groupBy(\"userId\", \"genre\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# 6. Pivot 해서 wide format으로 변환\n",
    "user_profile_df = user_genre_pref.groupBy(\"userId\").pivot(\"genre\").agg(avg(\"avg_rating\"))\n",
    "\n",
    "# 7. Null 값을 0으로 대체 (평점이 없는 장르는 0으로 간주)\n",
    "user_profile_df = user_profile_df.fillna(0)\n",
    "\n",
    "# 8. 컬럼명에 'pref_' 접두어 붙이기 (userId 제외)\n",
    "for col_name in user_profile_df.columns:\n",
    "    if col_name != \"userId\":\n",
    "        user_profile_df = user_profile_df.withColumnRenamed(col_name, f\"pref_{col_name}\")\n",
    "\n",
    "# 9. 앞서 정의한 MLflow 로깅이 포함된 KMeans 함수 호출\n",
    "result_df = run_kmeans_with_mlflow(user_profile_df, k=5, seed=42)\n",
    "\n",
    "# 10. 결과 확인 (예시)\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d86aed-d091-498a-85d9-23a7a4a00dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##평가지표_ Precision@10 , Recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a9e4c5a-aecc-4967-ae87-78abc4d0a218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_movies_by_user(userId: int):\n",
    "    # 1. 데이터 불러오기\n",
    "    df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "    df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "    df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "    df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\") \n",
    "\n",
    "    # 2. 영화 장르 전처리\n",
    "    movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    # 3. 사용자-장르 선호도 계산\n",
    "    ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    # 4. 벡터화 및 클러스터링\n",
    "    feature_cols = [col for col in user_profile.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "\n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\") \\\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "    # 5. 추천 로직 실행\n",
    "    user_seen_movies = df3.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    user_cluster = user_clusters.filter(col(\"userId\") == userId).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "\n",
    "    # 6. index 추가 및 결과 출력\n",
    "    indexed = top_recommendations.withColumn(\n",
    "        \"index\", row_number().over(Window.orderBy(col(\"movieId\"))) - 1\n",
    "    ).select(\"index\", \"movieId\")\n",
    "\n",
    "\n",
    "def evaluate_precision_recall_at_10(userId: int):\n",
    "    # Ensure recommend_movies_by_user returns a DataFrame\n",
    "    recommended_movies = recommend_movies_by_user(userId)\n",
    "    if recommended_movies is not None:\n",
    "        recommended_movies = recommended_movies.select(\"movieId\").limit(10)\n",
    "    else:\n",
    "        raise ValueError(f\"No recommendations found for userId: {userId}\")\n",
    "\n",
    "    # Actual movies liked by the user (rating 4.0 or higher)\n",
    "    relevant_movies = df3.filter(\n",
    "        (col(\"userId\") == userId) & (col(\"rating\") >= 4.0)\n",
    "    ).select(\"movieId\").distinct()\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    true_positives = recommended_movies.join(relevant_movies, \"movieId\").count()\n",
    "    precision = true_positives / recommended_movies.count()\n",
    "    recall = true_positives / relevant_movies.count()\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c03231-60a3-4abd-9fe9-046bb278cf68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 로딩 (이미 로딩되었다고 가정)\n",
    "# train, validation, test = ...\n",
    "\n",
    "# 2. 추천 함수 (train 데이터 기반)\n",
    "def recommend_movies_by_user_return_df(userId: int):\n",
    "    # 영화, 장르 데이터 로드\n",
    "    df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "    \n",
    "    # 장르 explode\n",
    "    movies_with_genres = df_movies.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "    \n",
    "    # train 데이터로 유저-장르 선호도 계산\n",
    "    ratings_with_genres = train.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "    \n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[c for c in user_profile.columns if c.startswith(\"pref_\")], outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "    \n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "    \n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    \n",
    "    ratings_with_cluster = train.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "    \n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df_movies.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "    \n",
    "    # user가 이미 본 영화 필터링 (train 기준)\n",
    "    user_seen_movies = train.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    \n",
    "    cluster_row = user_clusters.filter(col(\"userId\") == userId).collect()\n",
    "    if not cluster_row:\n",
    "        return None\n",
    "    user_cluster = cluster_row[0][\"prediction\"]\n",
    "    \n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "    \n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "    \n",
    "    return top_recommendations\n",
    "\n",
    "# 3. 평가지표 함수 (validation 또는 test 기준 실제 평점과 비교)\n",
    "def evaluate_precision_recall_safe(userId: int, top_k: int = 10):\n",
    "    try:\n",
    "        rec_df = recommend_movies_by_user_return_df(userId)\n",
    "    except Exception:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    if rec_df is None or rec_df.count() == 0:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    predicted = rec_df.select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # validation 또는 test 데이터에서 rating >= 4.0인 실제 선호 영화 추출\n",
    "    actual_df = validation.filter((col(\"userId\") == userId) & (col(\"rating\") >= 4.0)).select(\"movieId\").distinct()\n",
    "    actual = actual_df.rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    if len(actual) == 0:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    intersection = set(predicted) & set(actual)\n",
    "    precision = len(intersection) / top_k\n",
    "    recall = len(intersection) / len(actual)\n",
    "    \n",
    "    return {\"userId\": userId, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# 4. 평가 대상 사용자 선정 (validation 데이터 기준 rating 4점 이상 5개 이상)\n",
    "qualified_users_df = validation.filter(col(\"rating\") >= 4.0) \\\n",
    "    .groupBy(\"userId\") \\\n",
    "    .count() \\\n",
    "    .filter(\"count >= 5\") \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "user_ids = [row[\"userId\"] for row in qualified_users_df.collect()]\n",
    "\n",
    "# 5. 평가 수행 (상위 30명 기준)\n",
    "results = [evaluate_precision_recall_safe(uid) for uid in user_ids[:30]]\n",
    "\n",
    "# 6. 평균 정밀도 및 재현율 계산 및 출력\n",
    "df_results = pd.DataFrame(results)\n",
    "avg_precision = df_results[\"precision\"].mean()\n",
    "avg_recall = df_results[\"recall\"].mean()\n",
    "\n",
    "print(f\"▶ 평균 Precision@10: {avg_precision:.4f}\")\n",
    "print(f\"▶ 평균 Recall@10: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b2762d0-fcac-4cf8-b336-37d2f1c0e53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "541ee9c6-5d48-4df8-8819-bc30e7d84fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_kmeans_k(user_profile_df, k_min=2, k_max=10):\n",
    "    # 1. 벡터화\n",
    "    feature_cols = [col for col in user_profile_df.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features_df = assembler.transform(user_profile_df).select(\"userId\", \"features\")\n",
    "\n",
    "    # 2. Pandas로 변환 (features → vector → numpy)\n",
    "    features_np = user_features_df.select(\"features\").rdd.map(lambda x: x[0].toArray()).collect()\n",
    "    features_np = pd.DataFrame(features_np).values  # numpy array\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 3. 여러 k에 대해 평가\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(k=k, seed=42)\n",
    "        model = kmeans.fit(user_features_df)\n",
    "        predictions = model.transform(user_features_df)\n",
    "        \n",
    "        # 실루엣 스코어는 Pandas 기반으로 계산\n",
    "        preds = predictions.select(\"prediction\").rdd.map(lambda x: x[0]).collect()\n",
    "        score = silhouette_score(features_np, preds)\n",
    "\n",
    "        results.append({\"k\": k, \"silhouette_score\": score})\n",
    "        print(f\"✅ k={k}, silhouette_score={score:.4f}\")\n",
    "\n",
    "    # 4. 가장 높은 실루엣 점수를 가진 k 선택\n",
    "    best = max(results, key=lambda x: x[\"silhouette_score\"])\n",
    "    print(f\"\\n📌 최적 k: {best['k']} (Silhouette Score: {best['silhouette_score']:.4f})\")\n",
    "\n",
    "    return pd.DataFrame(results), best[\"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5ebae5-e931-455e-a6df-1efd7a02b2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, split, explode, avg, when, max as spark_max\n",
    "\n",
    "\n",
    "df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\")\n",
    "\n",
    "# 2. 영화 장르 분해 (movies.genres → 개별 장르 컬럼으로)\n",
    "movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "\n",
    "# 3. 모든 장르 리스트 추출\n",
    "distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# 4. 각 장르에 대해 binary 컬럼 생성 (해당 장르면 1, 아니면 0)\n",
    "for genre in distinct_genres:\n",
    "    movies_with_genres = movies_with_genres.withColumn(\n",
    "        f\"genre_{genre}\", when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# 5. 영화별 장르 one-hot encoding 집계\n",
    "genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "    *[spark_max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 6. 사용자 평점 데이터와 장르 결합\n",
    "ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# 7. 사용자별 장르 선호도 (평균) 계산 → 결과가 user_profile_df\n",
    "user_profile_df = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "    *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 8. KMeans 최적 군집 수 찾기 및 실행 (이 함수는 따로 정의되어 있어야 함)\n",
    "results_df, best_k = optimize_kmeans_k(user_profile_df, k_min=2, k_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de747787-47b1-4a81-92b3-832505a89370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Unity Catalog에서 데이터 불러오기\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "base_path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{base_path}.train_df\")\n",
    "validation = spark.read.table(f\"{base_path}.validation_df\")\n",
    "test = spark.read.table(f\"{base_path}.test_df\")\n",
    "\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\") \\\n",
    "                      .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "# 2. ALS 모델 정의 (파라미터 조정)\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "# 3. 모델 학습\n",
    "model = als.fit(train)\n",
    "\n",
    "# 4. 추천 대상 사용자 설정\n",
    "user_ids = [123]  # 하나의 사용자만 추천할 경우\n",
    "user_df = spark.createDataFrame([(uid,) for uid in user_ids], [\"userId\"])\n",
    "\n",
    "# 5. Top 50 추천 받아서 Top 10만 출력\n",
    "userRecs = model.recommendForUserSubset(user_df, 50)\n",
    "\n",
    "# 6. 추천 결과 정리\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "# 7. Top 10 영화만 index 부여하여 추출\n",
    "windowSpec = Window.partitionBy(\"userId\").orderBy(col(\"rating\").desc())\n",
    "topN = 10\n",
    "\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .filter(col(\"index\") < topN) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "# 8. 결과 출력\n",
    "display(indexedRecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ebee492-e293-49a7-82f7-28c5c4c0c921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr, size\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "def evaluate_precision_recall_at_10(model, test_df, df_movies):\n",
    "    # 1. 추천 대상 사용자만 추출\n",
    "    users = test_df.select(\"userId\").distinct()\n",
    "\n",
    "    # 2. 각 사용자에 대해 top-10 추천\n",
    "    userRecs = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "    # 3. 추천 결과 explode\n",
    "    recs = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                   .select(\"userId\", col(\"rec.movieId\").alias(\"movieId\"))\n",
    "\n",
    "    # 4. test_df에서 실제 본 영화 가져오기\n",
    "    test_actual = test_df.select(\"userId\", \"movieId\").distinct()\n",
    "\n",
    "    # 5. 추천 결과와 실제 값 비교 (True Positive)\n",
    "    hits = recs.join(test_actual, on=[\"userId\", \"movieId\"])\n",
    "\n",
    "    # 6. Precision@10 = (# hits) / 10\n",
    "    precision_per_user = hits.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"num_hits\") \\\n",
    "                             .withColumn(\"precision_at_10\", col(\"num_hits\") / 10.0)\n",
    "\n",
    "    # 7. Recall@10 = (# hits) / (# actual items in test set for that user)\n",
    "    actual_count = test_actual.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"actual_count\")\n",
    "    recall_per_user = hits.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"num_hits\") \\\n",
    "                          .join(actual_count, on=\"userId\") \\\n",
    "                          .withColumn(\"recall_at_10\", col(\"num_hits\") / col(\"actual_count\"))\n",
    "\n",
    "    # 8. 평균 Precision, Recall 계산\n",
    "    avg_precision = precision_per_user.agg({\"precision_at_10\": \"avg\"}).first()[0]\n",
    "    avg_recall = recall_per_user.agg({\"recall_at_10\": \"avg\"}).first()[0]\n",
    "\n",
    "    print(f\"📊 Precision@10: {avg_precision:.4f}\")\n",
    "    print(f\"📊 Recall@10:    {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91200fe2-c8f5-4b49-b6ad-03d03aa9cf2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluate_precision_recall_at_10(model, test, df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a6acb4-1409-4410-a87e-758f4e45f669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 지표 이름\n",
    "metrics = ['Precision@10', 'Recall@10']\n",
    "\n",
    "# 개선 전, 후 값\n",
    "before = [0.0100, 0.0002]\n",
    "after = [0.1080, 0.0744]\n",
    "\n",
    "x = np.arange(len(metrics))  # X축 위치\n",
    "width = 0.35                 # 바 너비\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar(x - width/2, before, width, label='Before', color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, after, width, label='After', color='skyblue')\n",
    "\n",
    "# 레이블 및 타이틀\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Performance Comparison: Before vs After')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# 값 표시\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 텍스트 오프셋\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.ylim(0, max(after) + 0.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "군집화",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
