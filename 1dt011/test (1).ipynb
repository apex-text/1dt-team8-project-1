{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f28631-32a7-467e-bb84-362eae5b19a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. êµ°ì§‘í™” ê¸°ë°˜ ì‚¬ìš©ì ì˜í™”ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66856272-6b6a-49ec-ab5c-62da4935fa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. ë°ì´í„° ë¡œë”©        â”‚\n",
    "â”‚ - movies.csv         â”‚\n",
    "â”‚ - ratings.csv        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2. ì¥ë¥´ ì „ì²˜ë¦¬                        â”‚\n",
    "â”‚ - ì¥ë¥´ explode ë° ì›-í•« ì¸ì½”ë”©         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 3. ì‚¬ìš©ì ì¥ë¥´ ì„ í˜¸ë„ ë²¡í„°í™”           â”‚\n",
    "â”‚ - ì¥ë¥´ x í‰ì  â†’ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 4. ì‚¬ìš©ì ë²¡í„° â†’ features   â”‚\n",
    "â”‚ - VectorAssembler ì‚¬ìš©     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 5. KMeans êµ°ì§‘í™”            â”‚\n",
    "â”‚ - ì‚¬ìš©ì êµ°ì§‘ ë¶„ë¥˜           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 6. í´ëŸ¬ìŠ¤í„° ë‚´ ì˜í™” í‰ê·  í‰ì  ê³„ì‚°    â”‚\n",
    "â”‚ - ê° êµ°ì§‘ ë‚´ ì¸ê¸° ì˜í™” íŒŒì•…          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 7. íŠ¹ì • ì‚¬ìš©ìì—ê²Œ ì¶”ì²œ ìˆ˜í–‰          â”‚\n",
    "â”‚ - ê°™ì€ í´ëŸ¬ìŠ¤í„° + ì•ˆ ë³¸ ì˜í™”          â”‚\n",
    "â”‚ - í‰ê·  í‰ì  ê¸°ë°˜ ìƒìœ„ Nê°œ ì¶”ì²œ        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 8. ì¶”ì²œ ê²°ê³¼ ì¶œë ¥                    â”‚\n",
    "â”‚ - movieId, title, genres, avg_ratingâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2328a1-ea33-47d4-ad69-a1b704a55b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_movies_by_user(userId: int):\n",
    "    catalog = \"1dt_team8_databricks\"\n",
    "    schema = \"final\"\n",
    "    path = f\"{catalog}.{schema}\"\n",
    "    \n",
    "    df1 = spark.read.table(f\"{path}.movies\")\n",
    "    df2 = spark.read.table(f\"{path}.links\")      \n",
    "    df3 = spark.read.table(f\"{path}.ratings\")\n",
    "    df4 = spark.read.table(f\"{path}.tags\")   \n",
    "\n",
    "    movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    feature_cols = [c for c in user_profile.columns if c.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "\n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\") \\\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "    user_seen_movies = df3.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    user_cluster = user_clusters.filter(col(\"userId\") == userId).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "\n",
    "    indexed = top_recommendations.withColumn(\n",
    "        \"index\", row_number().over(Window.orderBy(col(\"movieId\"))) - 1\n",
    "    ).select(\"index\", \"movieId\")\n",
    "\n",
    "    display(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b720df41-6bc1-41ae-b6cb-e07f80b2961d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recommend_movies_by_user(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6db313-bf1e-486f-8877-33f9ac9d1f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Flow ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f77b1c-f594-480d-9007-0db3c9100ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "def run_kmeans_with_mlflow(user_profile_df, k=5, seed=42):\n",
    "    # âœ… MLflow ì‹¤í—˜ ìœ„ì¹˜ ì„¤ì •\n",
    "    mlflow.set_experiment(\"/Users/1dt011@msacademy.msai.kr/1dt011\")\n",
    "\n",
    "    feature_cols = [col for col in user_profile_df.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile_df)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"KMeans_User_Clustering\"):\n",
    "        mlflow.log_param(\"k\", k)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "\n",
    "        kmeans = KMeans(k=k, seed=seed)\n",
    "        model = kmeans.fit(user_features)\n",
    "\n",
    "        # í´ëŸ¬ìŠ¤í„°ë§ ë¹„ìš©(Within Set Sum of Squared Errors)\n",
    "        mlflow.log_metric(\"wsse\", model.summary.trainingCost)\n",
    "\n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        mlflow.spark.log_model(model, \"kmeans_model\")\n",
    "\n",
    "        # userIdì™€ í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡ ê²°ê³¼ë§Œ ë°˜í™˜\n",
    "        return model.transform(user_features).select(\"userId\", \"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba48ee8-d895-4333-8538-002b0da00cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "movies_df = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "ratings_df = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "\n",
    "# 2. íƒ€ì… ë³€í™˜\n",
    "ratings_df = ratings_df.withColumn(\"movieId\", col(\"movieId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\")) \\\n",
    "                       .withColumn(\"userId\", col(\"userId\").cast(\"int\"))\n",
    "movies_df = movies_df.withColumn(\"movieId\", col(\"movieId\").cast(\"int\"))\n",
    "\n",
    "# 3. ì¥ë¥´ explode\n",
    "movie_genres = movies_df.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))).select(\"movieId\", \"genre\")\n",
    "\n",
    "# 4. í‰ì ê³¼ ì¥ë¥´ ì¡°ì¸\n",
    "ratings_with_genre = ratings_df.join(movie_genres, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# 5. ì‚¬ìš©ìë³„ ì¥ë¥´ë³„ í‰ê·  í‰ì  ê³„ì‚°\n",
    "user_genre_pref = ratings_with_genre.groupBy(\"userId\", \"genre\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# 6. Pivot í•´ì„œ wide formatìœ¼ë¡œ ë³€í™˜\n",
    "user_profile_df = user_genre_pref.groupBy(\"userId\").pivot(\"genre\").agg(avg(\"avg_rating\"))\n",
    "\n",
    "# 7. Null ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´ (í‰ì ì´ ì—†ëŠ” ì¥ë¥´ëŠ” 0ìœ¼ë¡œ ê°„ì£¼)\n",
    "user_profile_df = user_profile_df.fillna(0)\n",
    "\n",
    "# 8. ì»¬ëŸ¼ëª…ì— 'pref_' ì ‘ë‘ì–´ ë¶™ì´ê¸° (userId ì œì™¸)\n",
    "for col_name in user_profile_df.columns:\n",
    "    if col_name != \"userId\":\n",
    "        user_profile_df = user_profile_df.withColumnRenamed(col_name, f\"pref_{col_name}\")\n",
    "\n",
    "# 9. ì•ì„œ ì •ì˜í•œ MLflow ë¡œê¹…ì´ í¬í•¨ëœ KMeans í•¨ìˆ˜ í˜¸ì¶œ\n",
    "result_df = run_kmeans_with_mlflow(user_profile_df, k=5, seed=42)\n",
    "\n",
    "# 10. ê²°ê³¼ í™•ì¸ (ì˜ˆì‹œ)\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb7afa0-3b2e-4f07-ab7e-98a9f5763c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##í‰ê°€ì§€í‘œ_ Precision@10 , Recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bd26f5-de5c-4cb6-bfd4-9597273c7969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_movies_by_user(userId: int):\n",
    "    df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "    df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "    df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "    df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\") \n",
    "\n",
    "    movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    feature_cols = [col for col in user_profile.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "\n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\") \\\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "    user_seen_movies = df3.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    user_cluster = user_clusters.filter(col(\"userId\") == userId).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "\n",
    "    indexed = top_recommendations.withColumn(\n",
    "        \"index\", row_number().over(Window.orderBy(col(\"movieId\"))) - 1\n",
    "    ).select(\"index\", \"movieId\")\n",
    "\n",
    "\n",
    "def evaluate_precision_recall_at_10(userId: int):\n",
    "    recommended_movies = recommend_movies_by_user(userId)\n",
    "    if recommended_movies is not None:\n",
    "        recommended_movies = recommended_movies.select(\"movieId\").limit(10)\n",
    "    else:\n",
    "        raise ValueError(f\"No recommendations found for userId: {userId}\")\n",
    "\n",
    "    relevant_movies = df3.filter(\n",
    "        (col(\"userId\") == userId) & (col(\"rating\") >= 4.0)\n",
    "    ).select(\"movieId\").distinct()\n",
    "\n",
    "    true_positives = recommended_movies.join(relevant_movies, \"movieId\").count()\n",
    "    precision = true_positives / recommended_movies.count()\n",
    "    recall = true_positives / relevant_movies.count()\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aaea570-2653-4fa9-a32e-6b4beb0605a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "def recommend_movies_by_user_return_df(userId: int):\n",
    "    df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "    movies_with_genres = df_movies.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "    \n",
    "    ratings_with_genres = train.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "    \n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[c for c in user_profile.columns if c.startswith(\"pref_\")], outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "    \n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "    \n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    \n",
    "    ratings_with_cluster = train.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "    \n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df_movies.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "    \n",
    "    user_seen_movies = train.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    \n",
    "    cluster_row = user_clusters.filter(col(\"userId\") == userId).collect()\n",
    "    if not cluster_row:\n",
    "        return None\n",
    "    user_cluster = cluster_row[0][\"prediction\"]\n",
    "    \n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "    \n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "    \n",
    "    return top_recommendations\n",
    "\n",
    "def evaluate_precision_recall_safe(userId: int, top_k: int = 10):\n",
    "    try:\n",
    "        rec_df = recommend_movies_by_user_return_df(userId)\n",
    "    except Exception:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    if rec_df is None or rec_df.count() == 0:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    predicted = rec_df.select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    actual_df = validation.filter((col(\"userId\") == userId) & (col(\"rating\") >= 4.0)).select(\"movieId\").distinct()\n",
    "    actual = actual_df.rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    if len(actual) == 0:\n",
    "        return {\"userId\": userId, \"precision\": 0.0, \"recall\": 0.0}\n",
    "    \n",
    "    intersection = set(predicted) & set(actual)\n",
    "    precision = len(intersection) / top_k\n",
    "    recall = len(intersection) / len(actual)\n",
    "    \n",
    "    return {\"userId\": userId, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "qualified_users_df = validation.filter(col(\"rating\") >= 4.0) \\\n",
    "    .groupBy(\"userId\") \\\n",
    "    .count() \\\n",
    "    .filter(\"count >= 5\") \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "user_ids = [row[\"userId\"] for row in qualified_users_df.collect()]\n",
    "\n",
    "results = [evaluate_precision_recall_safe(uid) for uid in user_ids[:30]]\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "avg_precision = df_results[\"precision\"].mean()\n",
    "avg_recall = df_results[\"recall\"].mean()\n",
    "\n",
    "print(f\"â–¶ í‰ê·  Precision@10: {avg_precision:.4f}\")\n",
    "print(f\"â–¶ í‰ê·  Recall@10: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8bcee82-9d6a-44c9-899d-f1adb80bd9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8f3831-e281-4ab3-b37f-9509e2638998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_kmeans_k(user_profile_df, k_min=2, k_max=10):\n",
    "    feature_cols = [col for col in user_profile_df.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features_df = assembler.transform(user_profile_df).select(\"userId\", \"features\")\n",
    "\n",
    "    features_np = user_features_df.select(\"features\").rdd.map(lambda x: x[0].toArray()).collect()\n",
    "    features_np = pd.DataFrame(features_np).values  # numpy array\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(k=k, seed=42)\n",
    "        model = kmeans.fit(user_features_df)\n",
    "        predictions = model.transform(user_features_df)\n",
    "        preds = predictions.select(\"prediction\").rdd.map(lambda x: x[0]).collect()\n",
    "        score = silhouette_score(features_np, preds)\n",
    "\n",
    "        results.append({\"k\": k, \"silhouette_score\": score})\n",
    "        print(f\"âœ… k={k}, silhouette_score={score:.4f}\")\n",
    "\n",
    "    best = max(results, key=lambda x: x[\"silhouette_score\"])\n",
    "    print(f\"\\nğŸ“Œ ìµœì  k: {best['k']} (Silhouette Score: {best['silhouette_score']:.4f})\")\n",
    "\n",
    "    return pd.DataFrame(results), best[\"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef077b4-6a65-41d2-9193-c4d532b384a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, split, explode, avg, when, max as spark_max\n",
    "\n",
    "\n",
    "df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\")\n",
    "\n",
    "# 2. ì˜í™” ì¥ë¥´ ë¶„í•´ (movies.genres â†’ ê°œë³„ ì¥ë¥´ ì»¬ëŸ¼ìœ¼ë¡œ)\n",
    "movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "\n",
    "# 3. ëª¨ë“  ì¥ë¥´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# 4. ê° ì¥ë¥´ì— ëŒ€í•´ binary ì»¬ëŸ¼ ìƒì„± (í•´ë‹¹ ì¥ë¥´ë©´ 1, ì•„ë‹ˆë©´ 0)\n",
    "for genre in distinct_genres:\n",
    "    movies_with_genres = movies_with_genres.withColumn(\n",
    "        f\"genre_{genre}\", when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# 5. ì˜í™”ë³„ ì¥ë¥´ one-hot encoding ì§‘ê³„\n",
    "genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "    *[spark_max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 6. ì‚¬ìš©ì í‰ì  ë°ì´í„°ì™€ ì¥ë¥´ ê²°í•©\n",
    "ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# 7. ì‚¬ìš©ìë³„ ì¥ë¥´ ì„ í˜¸ë„ (í‰ê· ) ê³„ì‚° â†’ ê²°ê³¼ê°€ user_profile_df\n",
    "user_profile_df = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "    *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 8. KMeans ìµœì  êµ°ì§‘ ìˆ˜ ì°¾ê¸° ë° ì‹¤í–‰ (ì´ í•¨ìˆ˜ëŠ” ë”°ë¡œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)\n",
    "results_df, best_k = optimize_kmeans_k(user_profile_df, k_min=2, k_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b7d2db-cf51-4d3b-ad9c-adc27a6ade9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Unity Catalogì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "base_path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{base_path}.train_df\")\n",
    "validation = spark.read.table(f\"{base_path}.validation_df\")\n",
    "test = spark.read.table(f\"{base_path}.test_df\")\n",
    "\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\") \\\n",
    "                      .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "# 2. ALS ëª¨ë¸ ì •ì˜ (íŒŒë¼ë¯¸í„° ì¡°ì •)\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "# 3. ëª¨ë¸ í•™ìŠµ\n",
    "model = als.fit(train)\n",
    "\n",
    "# 4. ì¶”ì²œ ëŒ€ìƒ ì‚¬ìš©ì ì„¤ì •\n",
    "user_ids = [123]  # í•˜ë‚˜ì˜ ì‚¬ìš©ìë§Œ ì¶”ì²œí•  ê²½ìš°\n",
    "user_df = spark.createDataFrame([(uid,) for uid in user_ids], [\"userId\"])\n",
    "\n",
    "# 5. Top 50 ì¶”ì²œ ë°›ì•„ì„œ Top 10ë§Œ ì¶œë ¥\n",
    "userRecs = model.recommendForUserSubset(user_df, 50)\n",
    "\n",
    "# 6. ì¶”ì²œ ê²°ê³¼ ì •ë¦¬\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "# 7. Top 10 ì˜í™”ë§Œ index ë¶€ì—¬í•˜ì—¬ ì¶”ì¶œ\n",
    "windowSpec = Window.partitionBy(\"userId\").orderBy(col(\"rating\").desc())\n",
    "topN = 10\n",
    "\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .filter(col(\"index\") < topN) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "# 8. ê²°ê³¼ ì¶œë ¥\n",
    "display(indexedRecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee5c9b2-c9d3-467e-8847-b6dbb6120e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr, size\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "def evaluate_precision_recall_at_10(model, test_df, df_movies):\n",
    "    # 1. ì¶”ì²œ ëŒ€ìƒ ì‚¬ìš©ìë§Œ ì¶”ì¶œ\n",
    "    users = test_df.select(\"userId\").distinct()\n",
    "\n",
    "    # 2. ê° ì‚¬ìš©ìì— ëŒ€í•´ top-10 ì¶”ì²œ\n",
    "    userRecs = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "    # 3. ì¶”ì²œ ê²°ê³¼ explode\n",
    "    recs = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                   .select(\"userId\", col(\"rec.movieId\").alias(\"movieId\"))\n",
    "\n",
    "    # 4. test_dfì—ì„œ ì‹¤ì œ ë³¸ ì˜í™” ê°€ì ¸ì˜¤ê¸°\n",
    "    test_actual = test_df.select(\"userId\", \"movieId\").distinct()\n",
    "\n",
    "    # 5. ì¶”ì²œ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ ë¹„êµ (True Positive)\n",
    "    hits = recs.join(test_actual, on=[\"userId\", \"movieId\"])\n",
    "\n",
    "    # 6. Precision@10 = (# hits) / 10\n",
    "    precision_per_user = hits.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"num_hits\") \\\n",
    "                             .withColumn(\"precision_at_10\", col(\"num_hits\") / 10.0)\n",
    "\n",
    "    # 7. Recall@10 = (# hits) / (# actual items in test set for that user)\n",
    "    actual_count = test_actual.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"actual_count\")\n",
    "    recall_per_user = hits.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"num_hits\") \\\n",
    "                          .join(actual_count, on=\"userId\") \\\n",
    "                          .withColumn(\"recall_at_10\", col(\"num_hits\") / col(\"actual_count\"))\n",
    "\n",
    "    # 8. í‰ê·  Precision, Recall ê³„ì‚°\n",
    "    avg_precision = precision_per_user.agg({\"precision_at_10\": \"avg\"}).first()[0]\n",
    "    avg_recall = recall_per_user.agg({\"recall_at_10\": \"avg\"}).first()[0]\n",
    "\n",
    "    print(f\"ğŸ“Š Precision@10: {avg_precision:.4f}\")\n",
    "    print(f\"ğŸ“Š Recall@10:    {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc8a639-fba6-48f1-a7ac-6d4202eb275f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluate_precision_recall_at_10(model, test, df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7542b8f-3f8b-40cc-a924-afef1956672a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ì§€í‘œ ì´ë¦„\n",
    "metrics = ['Precision@10', 'Recall@10']\n",
    "\n",
    "# ê°œì„  ì „, í›„ ê°’\n",
    "before = [0.0100, 0.0002]\n",
    "after = [0.1080, 0.0744]\n",
    "\n",
    "x = np.arange(len(metrics))  # Xì¶• ìœ„ì¹˜\n",
    "width = 0.35                 # ë°” ë„ˆë¹„\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars1 = ax.bar(x - width/2, before, width, label='Before', color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, after, width, label='After', color='skyblue')\n",
    "\n",
    "# ë ˆì´ë¸” ë° íƒ€ì´í‹€\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Performance Comparison: Before vs After')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # í…ìŠ¤íŠ¸ ì˜¤í”„ì…‹\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.ylim(0, max(after) + 0.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefdd18e-9e71-461f-a236-ac65fb176d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. ALS ê¸°ë°˜ ì‚¬ìš©ì ì˜í™” ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da65b0c7-250d-4165-ad20-beae277c6dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        [ì‚¬ìš©ì]\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "     â”‚  ì„¤ë¬¸ ì‘ë‹µ â”‚  â† ì‚¬ìš©ìì˜ ì·¨í–¥(ì¥ë¥´ ë“±)\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚ì‚¬ìš©ì ì„±í–¥ ë²¡í„° ìƒì„±â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "[ ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ ]  â† Cosine Similarity ë“±\n",
    "\n",
    "           â–²\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "     â”‚ í‰ì  ë°ì´í„° â”‚  â† ratings.csv\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼\n",
    "   [ ALS ì¶”ì²œ ëª¨ë¸ ]  â† Spark ML ALS\n",
    "\n",
    "           â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ Hybrid ì¶”ì²œ ì¡°í•©â”‚ â† ì„¤ë¬¸ ê¸°ë°˜ + ALS ê¸°ë°˜ ê²°í•© (ê°€ì¤‘ í‰ê·  ë“±)\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼\n",
    "     [ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸]\n",
    "           â–¼\n",
    "       [ì‹œê°í™”/ì¶œë ¥]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a142bcf4-ae13-493e-acc3-362fc0a30c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "base_path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{base_path}.train_df\")\n",
    "validation = spark.read.table(f\"{base_path}.validation_df\")\n",
    "test = spark.read.table(f\"{base_path}.test_df\")\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\") \\\n",
    "                      .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "user_df = spark.createDataFrame([(123,)], [\"userId\"])\n",
    "userRecs = model.recommendForUserSubset(user_df, 10)\n",
    "\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "windowSpec = Window.orderBy(col(\"rating\").desc())\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "display(indexedRecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5770d6f-9e29-4dd0-9bb4-26b7152a5218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Flow ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6ca01c-9778-4ced-b332-ddc55a96bc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 0. ì‹¤í—˜ ì„¤ì •\n",
    "mlflow.set_experiment('/Users/1dt011@msacademy.msai.kr/1dt011')\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë”© (Unity Catalog ì‚¬ìš©)\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{path}.train_df\")\n",
    "test = spark.read.table(f\"{path}.test_df\")\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\") \\\n",
    "                      .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "# 2. MLflow ì‹¤í—˜ ì‹œì‘\n",
    "with mlflow.start_run(run_name=\"ALS-Recommender-UnityCatalog\"):\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    rank = 10\n",
    "    maxIter = 10\n",
    "    regParam = 0.1\n",
    "\n",
    "    # ALS ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "    als = ALS(\n",
    "        userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "        rank=rank, maxIter=maxIter, regParam=regParam,\n",
    "        coldStartStrategy=\"drop\", nonnegative=True\n",
    "    )\n",
    "\n",
    "    model = als.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "\n",
    "    # í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # MLflow ë¡œê¹…\n",
    "    mlflow.log_param(\"rank\", rank)\n",
    "    mlflow.log_param(\"maxIter\", maxIter)\n",
    "    mlflow.log_param(\"regParam\", regParam)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    mlflow.spark.log_model(model, \"als_model\")\n",
    "\n",
    "    print(f\"âœ… MLflow Run Completed - RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c7095c-9e88-49ca-96a4-e0c2f5df2c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## í‰ê°€ì§€í‘œ_ Precision@10 , Recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19014ba6-7887-4e28-9511-b357c06f43ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, collect_set, size, array_intersect\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{path}.train_df\")\n",
    "test = spark.read.table(f\"{path}.test_df\")\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\", nonnegative=True\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "user_df = spark.createDataFrame([(123,)], [\"userId\"])\n",
    "userRecs = model.recommendForUserSubset(user_df, 10)\n",
    "\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "windowSpec = Window.orderBy(col(\"rating\").desc())\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "positive_test = test.filter(col(\"rating\") >= 4.0) \\\n",
    "                    .groupBy(\"userId\") \\\n",
    "                    .agg(collect_set(\"movieId\").alias(\"true_items\"))\n",
    "\n",
    "userRecsAll = model.recommendForAllUsers(10)\n",
    "\n",
    "predicted_items = userRecsAll.select(\"userId\", \n",
    "    expr(\"transform(recommendations, x -> x.movieId)\").alias(\"pred_items\")\n",
    ")\n",
    "\n",
    "joined = predicted_items.join(positive_test, on=\"userId\")\n",
    "\n",
    "metrics = joined.withColumn(\"num_relevant_and_recommended\", \n",
    "                                size(array_intersect(\"pred_items\", \"true_items\"))) \\\n",
    "                .withColumn(\"precision_at_10\", \n",
    "                                col(\"num_relevant_and_recommended\") / expr(\"size(pred_items)\")) \\\n",
    "                .withColumn(\"recall_at_10\", \n",
    "                                col(\"num_relevant_and_recommended\") / expr(\"size(true_items)\"))\n",
    "\n",
    "precision_recall = metrics.selectExpr(\"avg(precision_at_10) as avg_precision_at_10\", \n",
    "                                      \"avg(recall_at_10) as avg_recall_at_10\")\n",
    "\n",
    "precision_recall.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "593c1a8e-bb2e-46fa-840b-91f4634222c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**ê°œì„ **\n",
    "âœ… ê°œì„  í¬ì¸íŠ¸ ìš”ì•½\n",
    "í•­ëª©\të³€ê²½ ì „\të³€ê²½ í›„ ì œì•ˆ\n",
    "\n",
    "ALS íŒŒë¼ë¯¸í„°\tê¸°ë³¸ê°’ ì‚¬ìš©\trank, maxIter, regParam ì¡°ì •\n",
    "\n",
    "ì¶”ì²œ ìˆ˜\tTop 10\tTop 50ìœ¼ë¡œ ëŠ˜ë¦¬ê³  ê·¸ì¤‘ ìƒìœ„ Nê°œ í‰ê°€\n",
    "\n",
    "ì¶”ì²œ ìœ ì €\tê³ ì • userId\të‹¤ìˆ˜ ìœ ì €ì— ëŒ€í•´ í‰ê°€ ìë™í™” ê°€ëŠ¥í•˜ê²Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066de2c9-03c0-4ec7-8a92-8fa8525eb1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"final\"\n",
    "base_path = f\"{catalog}.{schema}\"\n",
    "\n",
    "train = spark.read.table(f\"{base_path}.train_df\")\n",
    "validation = spark.read.table(f\"{base_path}.validation_df\")\n",
    "test = spark.read.table(f\"{base_path}.test_df\")\n",
    "\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\") \\\n",
    "                      .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "user_ids = [123]\n",
    "user_df = spark.createDataFrame([(uid,) for uid in user_ids], [\"userId\"])\n",
    "\n",
    "userRecs = model.recommendForUserSubset(user_df, 50)\n",
    "\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "windowSpec = Window.partitionBy(\"userId\").orderBy(col(\"rating\").desc())\n",
    "topN = 10\n",
    "\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .filter(col(\"index\") < topN) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "display(indexedRecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98bd08a-df0b-4dcd-9921-b03b7192d781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, collect_set, size\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. ì¶”ì²œ ê²°ê³¼(topNRecs)ì—ëŠ” userId, movieIdê°€ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "#    topNRecs: ALSë¡œ ì¶”ì²œëœ Top 10 movie per user\n",
    "\n",
    "# 2. ì‹¤ì œ í‰ê°€ ë°ì´í„°ì—ì„œ í‰ì  4.0 ì´ìƒì¸ ì˜í™”ë§Œ ê¸ì •ì ìœ¼ë¡œ ê°„ì£¼\n",
    "actual_relevant = test.filter(col(\"rating\") >= 4.0) \\\n",
    "                      .select(\"userId\", \"movieId\") \\\n",
    "                      .distinct() \\\n",
    "                      .groupBy(\"userId\") \\\n",
    "                      .agg(collect_set(\"movieId\").alias(\"actual_movies\"))\n",
    "\n",
    "# 3. ì¶”ì²œ ê²°ê³¼ë¥¼ userIdë³„ë¡œ movieId ë¦¬ìŠ¤íŠ¸ë¡œ ì§‘ê³„\n",
    "predicted_recs = topNRecs.groupBy(\"userId\") \\\n",
    "                         .agg(collect_set(\"movieId\").alias(\"predicted_movies\"))\n",
    "\n",
    "# 4. ì‹¤ì œ/ì˜ˆì¸¡ join\n",
    "joined = predicted_recs.join(actual_relevant, on=\"userId\", how=\"inner\")\n",
    "\n",
    "# 5. Precision@10, Recall@10 ê³„ì‚°\n",
    "def precision_recall_udf(predicted, actual):\n",
    "    predicted_set = set(predicted)\n",
    "    actual_set = set(actual)\n",
    "    intersection = predicted_set & actual_set\n",
    "    precision = len(intersection) / len(predicted_set) if predicted_set else 0.0\n",
    "    recall = len(intersection) / len(actual_set) if actual_set else 0.0\n",
    "    return (precision, recall)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"precision\", DoubleType(), True),\n",
    "    StructField(\"recall\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "precision_recall_udf_spark = udf(precision_recall_udf, schema)\n",
    "\n",
    "# 6. ì»¬ëŸ¼ ìƒì„±\n",
    "scored = joined.withColumn(\"metrics\", precision_recall_udf_spark(col(\"predicted_movies\"), col(\"actual_movies\"))) \\\n",
    "               .select(\"userId\", \"metrics.*\")\n",
    "\n",
    "# 7. í‰ê·  ê³„ì‚°\n",
    "avg_scores = scored.select(F.avg(\"precision\").alias(\"avg_precision_at_10\"),\n",
    "                           F.avg(\"recall\").alias(\"avg_recall_at_10\"))\n",
    "\n",
    "# 8. ì¶œë ¥\n",
    "display(avg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267418a6-493a-4571-955f-106a810cf7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ì§€í‘œ ì´ë¦„\n",
    "metrics = ['Precision@10', 'Recall@10']\n",
    "\n",
    "# ê°œì„  ì „, í›„ ê°’\n",
    "before = [0.00387, 0.00422]\n",
    "after = [0.01356, 0.01423]\n",
    "\n",
    "x = np.arange(len(metrics))  # Xì¶• ìœ„ì¹˜\n",
    "width = 0.35                 # ë°” ë„ˆë¹„\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bars1 = ax.bar(x - width/2, before, width, label='Before', color='lightcoral')\n",
    "bars2 = ax.bar(x + width/2, after, width, label='After', color='skyblue')\n",
    "\n",
    "# ë ˆì´ë¸” ë° íƒ€ì´í‹€\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Avg Precision@10 and Recall@10 (Before vs After)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# ê°’ í‘œì‹œ (ì†Œìˆ˜ì  3ìë¦¬)\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # í…ìŠ¤íŠ¸ ì˜¤í”„ì…‹\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.ylim(0, max(after) + 0.005)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8846788788865742,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test (1)",
   "widgets": {
    "maxIter": {
     "currentValue": "10",
     "nuid": "e0daffc2-7f59-46a6-98a1-f5c3cb1b6560",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "maxIter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "maxIter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rank": {
     "currentValue": "10",
     "nuid": "2493609f-1949-4523-bfb7-b0c0615f9c7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "rank",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "rank",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "regParam": {
     "currentValue": "0.1",
     "nuid": "81f5edf8-89fd-402e-92a4-001760157f8a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.1",
      "label": null,
      "name": "regParam",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.1",
      "label": null,
      "name": "regParam",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
