{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca20101-8dcc-47df-aa2c-12b1f8249aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ALS ê¸°ë°˜ ì „ì²´ ì‚¬ìš©ì ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef0b2f7-d101-43e5-80ba-2c6a4f2ae758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ALSì„ í™œìš©í•œ ì „ì²´ ì‚¬ìš©ì\n",
    "#ALS (Alternating Least Squares)ëŠ” ì‚¬ìš©ì-ì•„ì´í…œ í‰ì  í–‰ë ¬(Rating Matrix)ì„ ë¶„í•´í•´ì„œ, ì‚¬ìš©ìì™€ ì•„ì´í…œì˜ ì ì¬ ìš”ì¸(latent factor)ì„ í•™ìŠµ\n",
    "#ëŒ€ìš©ëŸ‰ì— ê°•í•¨ (Spark ê¸°ë°˜ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬)\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\")\n",
    "\n",
    "# âœ… í•„ìš”í•œ ì»¬ëŸ¼ ìºìŠ¤íŒ… (ì •ìˆ˜í˜•/ì‹¤ìˆ˜í˜•)\n",
    "ratings = df3.select(\n",
    "    col(\"userId\").cast(\"integer\"),\n",
    "    col(\"movieId\").cast(\"integer\"),\n",
    "    col(\"rating\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "(training, test) = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS ëª¨ë¸ ì •ì˜\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",  # NaN ì˜ˆì¸¡ ì œê±°\n",
    "    nonnegative=True\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = als.fit(training)\n",
    "\n",
    "# ì „ì²´ ì‚¬ìš©ì ì¶”ì²œ (Top 5)\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "display(userRecs)\n",
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì(ì˜ˆ: 123)ì— ëŒ€í•œ ì¶”ì²œ\n",
    "user_df = spark.createDataFrame([(123,)], [\"userId\"])\n",
    "userRecs = model.recommendForUserSubset(user_df, 5)\n",
    "display(userRecs)\n",
    "\n",
    "# ì¶”ì²œ ê²°ê³¼ ë¶„í•´ ë° ì˜í™” ì •ë³´ ë¶™ì´ê¸°\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                           .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "# âœ… df1ì„ ì˜í™” ì •ë³´ ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "movies = df1.select(\n",
    "    col(\"movieId\").cast(\"integer\"),\n",
    "    \"title\",\n",
    "    \"genres\"\n",
    ")\n",
    "\n",
    "# ì¶”ì²œ ì˜í™”ì— ì œëª© ë¶™ì´ê¸°\n",
    "recommended_movies = userRecsExploded.join(movies, on=\"movieId\", how=\"inner\")\n",
    "display(recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a8d0d887-ffc9-41de-a64c-243c3b372bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ìì˜ ì„¤ë¬¸ ì‘ë‹µ ì˜ˆì‹œ (ì„ í˜¸ ì¥ë¥´) \n",
    "user_selected_genres = ['Action', 'Sci-Fi', 'Thriller']\n",
    "\n",
    "# ì˜í™” ì¥ë¥´ ë°ì´í„° ì¤€ë¹„\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Spark â†’ Pandas ë³€í™˜\n",
    "movies_pd = df1.select(\"movieId\", \"title\", \"genres\").dropna().toPandas()\n",
    "movies_pd['genres'] = movies_pd['genres'].apply(lambda x: x.split('|'))\n",
    "\n",
    "# ì¥ë¥´ ì›-í•« ì¸ì½”ë”©\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_matrix = mlb.fit_transform(movies_pd['genres'])\n",
    "\n",
    "# ì‚¬ìš©ì ì¥ë¥´ ë²¡í„° ìƒì„±\n",
    "user_vec = [1 if genre in user_selected_genres else 0 for genre in mlb.classes_]\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚° (Cosine Similarity)\n",
    "similarities = cosine_similarity([user_vec], genre_matrix)[0]\n",
    "movies_pd['similarity'] = similarities\n",
    "\n",
    "# ìƒìœ„ ì¶”ì²œ ì˜í™” Top 10\n",
    "recommended = movies_pd.sort_values(by='similarity', ascending=False).head(10)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "display(recommended[['title', 'genres', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "89dbf2de-1bb9-4ea5-a7a8-4e841f1480cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, desc\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë”© (ì´ë¯¸ ì‹¤í–‰ë˜ì—ˆì„ ê²½ìš° ìƒëµ ê°€ëŠ¥)\n",
    "movies_df = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/movies.csv\").withColumn(\"movieId\", col(\"movieId\").cast(\"int\"))\n",
    "ratings_df = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/ratings.csv\") \\\n",
    "                       .withColumn(\"userId\", col(\"userId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"movieId\", col(\"movieId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "\n",
    "# ğŸ”§ ì‚¬ìš©ì ID ì„¤ì • (ì›í•˜ëŠ” ì‚¬ìš©ì IDë¥¼ ë„£ìœ¼ì„¸ìš”)\n",
    "target_user_id = 123  # ì˜ˆì‹œ\n",
    "\n",
    "# 2. í•´ë‹¹ ì‚¬ìš©ìê°€ ë†’ê²Œ í‰ê°€í•œ ì˜í™” ì¶”ì¶œ (ì˜ˆ: í‰ì  4 ì´ìƒ)\n",
    "high_rated_movies = ratings_df.filter((col(\"userId\") == target_user_id) & (col(\"rating\") >= 4.0))\n",
    "\n",
    "# 3. ê·¸ ì˜í™”ë“¤ì˜ ì¥ë¥´ ì¶”ì¶œ\n",
    "movies_with_genres = movies_df.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "\n",
    "user_preferred_genres = high_rated_movies.join(movies_with_genres, on=\"movieId\") \\\n",
    "                                         .select(\"genre\") \\\n",
    "                                         .distinct()\n",
    "\n",
    "# 4. ì „ì²´ ì˜í™” ì¤‘ í•´ë‹¹ ì¥ë¥´ ì˜í™”ë“¤ ì¤‘ í‰ì ì´ ë†’ì€ ì˜í™” êµ¬í•˜ê¸°\n",
    "# í•´ë‹¹ ì¥ë¥´ ì˜í™” ì „ì²´ ì¶”ì¶œ\n",
    "genre_movies = movies_with_genres.join(user_preferred_genres, on=\"genre\")\n",
    "\n",
    "# ì˜í™” í‰ê·  í‰ì  ê³„ì‚°\n",
    "movie_avg_ratings = ratings_df.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# ì¥ë¥´ë³„ ì˜í™” + í‰ì  ê²°í•©\n",
    "genre_movies_with_ratings = genre_movies.join(movie_avg_ratings, on=\"movieId\", how=\"left\") \\\n",
    "                                        .select(\"movieId\", \"title\", \"genres\", \"avg_rating\") \\\n",
    "                                        .dropDuplicates([\"movieId\"])\n",
    "\n",
    "# 5. ì‚¬ìš©ìê°€ ì´ë¯¸ ë³¸ ì˜í™” ì œì™¸\n",
    "seen_movies = ratings_df.filter(col(\"userId\") == target_user_id).select(\"movieId\").distinct()\n",
    "\n",
    "recommendations = genre_movies_with_ratings.join(seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "# 6. ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ì¶”ì²œ\n",
    "top_n_recommendations = recommendations.orderBy(desc(\"avg_rating\")).limit(10)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "top_n_recommendations.select(\"movieId\", \"title\", \"genres\", \"avg_rating\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f28631-32a7-467e-bb84-362eae5b19a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## êµ°ì§‘í™” ê¸°ë°˜ ì‚¬ìš©ì ì˜í™”ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66856272-6b6a-49ec-ab5c-62da4935fa96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. ë°ì´í„° ë¡œë”©        â”‚\n",
    "â”‚ - movies.csv         â”‚\n",
    "â”‚ - ratings.csv        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2. ì¥ë¥´ ì „ì²˜ë¦¬                        â”‚\n",
    "â”‚ - ì¥ë¥´ explode ë° ì›-í•« ì¸ì½”ë”©         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 3. ì‚¬ìš©ì ì¥ë¥´ ì„ í˜¸ë„ ë²¡í„°í™”           â”‚\n",
    "â”‚ - ì¥ë¥´ x í‰ì  â†’ ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 4. ì‚¬ìš©ì ë²¡í„° â†’ features   â”‚\n",
    "â”‚ - VectorAssembler ì‚¬ìš©     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 5. KMeans êµ°ì§‘í™”            â”‚\n",
    "â”‚ - ì‚¬ìš©ì êµ°ì§‘ ë¶„ë¥˜           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 6. í´ëŸ¬ìŠ¤í„° ë‚´ ì˜í™” í‰ê·  í‰ì  ê³„ì‚°    â”‚\n",
    "â”‚ - ê° êµ°ì§‘ ë‚´ ì¸ê¸° ì˜í™” íŒŒì•…          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 7. íŠ¹ì • ì‚¬ìš©ìì—ê²Œ ì¶”ì²œ ìˆ˜í–‰          â”‚\n",
    "â”‚ - ê°™ì€ í´ëŸ¬ìŠ¤í„° + ì•ˆ ë³¸ ì˜í™”          â”‚\n",
    "â”‚ - í‰ê·  í‰ì  ê¸°ë°˜ ìƒìœ„ Nê°œ ì¶”ì²œ        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 8. ì¶”ì²œ ê²°ê³¼ ì¶œë ¥                    â”‚\n",
    "â”‚ - movieId, title, genres, avg_ratingâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2328a1-ea33-47d4-ad69-a1b704a55b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, explode, avg, when, max, row_number\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def recommend_movies_by_user(userId: int):\n",
    "    # 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df1 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "    df2 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.links\")\n",
    "    df3 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "    df4 = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.tags\") \n",
    "\n",
    "    # 2. ì˜í™” ì¥ë¥´ ì „ì²˜ë¦¬\n",
    "    movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "    distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    for genre in distinct_genres:\n",
    "        movies_with_genres = movies_with_genres.withColumn(\n",
    "            f\"genre_{genre}\",\n",
    "            when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "    genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "        *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    # 3. ì‚¬ìš©ì-ì¥ë¥´ ì„ í˜¸ë„ ê³„ì‚°\n",
    "    ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "    user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "        *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    "    )\n",
    "\n",
    "    # 4. ë²¡í„°í™” ë° í´ëŸ¬ìŠ¤í„°ë§\n",
    "    feature_cols = [col for col in user_profile.columns if col.startswith(\"pref_\")]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    user_features = assembler.transform(user_profile)\n",
    "\n",
    "    kmeans = KMeans(k=5, seed=42)\n",
    "    model = kmeans.fit(user_features)\n",
    "\n",
    "    user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "    ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "    movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\") \\\n",
    "        .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "    movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "    # 5. ì¶”ì²œ ë¡œì§ ì‹¤í–‰\n",
    "    user_seen_movies = df3.filter(col(\"userId\") == userId).select(\"movieId\").distinct()\n",
    "    user_cluster = user_clusters.filter(col(\"userId\") == userId).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "    recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "    recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "    top_recommendations = recommend_pool_unseen \\\n",
    "        .filter(col(\"genres\") != \"(no genres listed)\") \\\n",
    "        .orderBy(col(\"avg_rating\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .select(\"movieId\")\n",
    "\n",
    "    # 6. index ì¶”ê°€ ë° ê²°ê³¼ ì¶œë ¥\n",
    "    indexed = top_recommendations.withColumn(\n",
    "        \"index\", row_number().over(Window.orderBy(col(\"movieId\"))) - 1\n",
    "    ).select(\"index\", \"movieId\")\n",
    "\n",
    "    display(indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b720df41-6bc1-41ae-b6cb-e07f80b2961d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "recommend_movies_by_user(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3c457fb0-74a5-4bc1-9f9c-7f84dc31f8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#êµ°ì§‘í™”ë¥¼ í™œìš©í•œ ê·¸ ì‚¬ëŒì´ ì¢‹ì•„í•  ë§Œí•œ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "from pyspark.sql.functions import col, split, explode, avg, when, max\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df1 = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/movies.csv\").withColumn(\"movieId\", col(\"movieId\").cast(\"int\"))\n",
    "df3 = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/ratings.csv\") \\\n",
    "               .withColumn(\"userId\", col(\"userId\").cast(\"int\")) \\\n",
    "               .withColumn(\"movieId\", col(\"movieId\").cast(\"int\")) \\\n",
    "               .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "\n",
    "# 2. ì˜í™” ì¥ë¥´ ì „ì²˜ë¦¬\n",
    "movies_with_genres = df1.withColumn(\"genre\", explode(split(\"genres\", \"\\\\|\")))\n",
    "distinct_genres = movies_with_genres.select(\"genre\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# ì¥ë¥´ ì›-í•« ì¸ì½”ë”©\n",
    "for genre in distinct_genres:\n",
    "    movies_with_genres = movies_with_genres.withColumn(\n",
    "        f\"genre_{genre}\",\n",
    "        when(col(\"genre\") == genre, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# ì˜í™”ë³„ ì¥ë¥´ ë²¡í„° ì™„ì„±\n",
    "genre_features = movies_with_genres.groupBy(\"movieId\").agg(\n",
    "    *[max(f\"genre_{genre}\").alias(f\"genre_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 3. ì‚¬ìš©ì-ì¥ë¥´ ì„ í˜¸ë„ ê³„ì‚°\n",
    "ratings_with_genres = df3.join(genre_features, on=\"movieId\", how=\"inner\")\n",
    "\n",
    "user_profile = ratings_with_genres.groupBy(\"userId\").agg(\n",
    "    *[avg(f\"genre_{genre}\").alias(f\"pref_{genre}\") for genre in distinct_genres]\n",
    ")\n",
    "\n",
    "# 4. KMeansë¥¼ ìœ„í•œ ë²¡í„°í™”\n",
    "feature_cols = [col for col in user_profile.columns if col.startswith(\"pref_\")]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "user_features = assembler.transform(user_profile)\n",
    "\n",
    "# 5. KMeans í´ëŸ¬ìŠ¤í„°ë§\n",
    "kmeans = KMeans(k=5, seed=42)\n",
    "model = kmeans.fit(user_features)\n",
    "\n",
    "# 6. ì‚¬ìš©ì í´ëŸ¬ìŠ¤í„° ê²°ê³¼\n",
    "user_clusters = model.transform(user_features).select(\"userId\", \"prediction\")\n",
    "\n",
    "# 7. í´ëŸ¬ìŠ¤í„°ë³„ ì¸ê¸° ì˜í™” ì¶”ì²œ (í‰ì  í‰ê·  ê¸°ì¤€)\n",
    "ratings_with_cluster = df3.join(user_clusters, on=\"userId\")\n",
    "movie_avg_by_cluster = ratings_with_cluster.groupBy(\"prediction\", \"movieId\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# ì˜í™” ì œëª© ë¶™ì´ê¸°\n",
    "movie_avg_with_titles = movie_avg_by_cluster.join(df1.select(\"movieId\", \"title\", \"genres\"), on=\"movieId\")\n",
    "\n",
    "# 8. ì‚¬ìš©ìê°€ ë³´ì§€ ì•Šì€ ì˜í™”ë§Œ ì¶”ì²œ\n",
    "target_user_id = 2  # <- ì¶”ì²œ ëŒ€ìƒ ì‚¬ìš©ì ID ì…ë ¥\n",
    "\n",
    "# ì¶”ì²œ ì˜í™” ì¤‘ ì¥ë¥´ê°€ ì—†ëŠ” ì˜í™” ì œì™¸\n",
    "top_recommendations_filtered = top_recommendations.filter(col(\"genres\") != \"(no genres listed)\")\n",
    "top_recommendations_filtered.select(\"movieId\", \"title\", \"genres\", \"avg_rating\").show(truncate=False)\n",
    "\n",
    "user_seen_movies = df3.filter(col(\"userId\") == target_user_id).select(\"movieId\").distinct()\n",
    "user_cluster = user_clusters.filter(col(\"userId\") == target_user_id).select(\"prediction\").collect()[0][0]\n",
    "\n",
    "recommend_pool = movie_avg_with_titles.filter(col(\"prediction\") == user_cluster)\n",
    "recommend_pool_unseen = recommend_pool.join(user_seen_movies, on=\"movieId\", how=\"left_anti\")\n",
    "\n",
    "# 9. ìƒìœ„ Nê°œ ì¶”ì²œ (ì˜ˆ: 5ê°œ)\n",
    "top_recommendations = recommend_pool_unseen.orderBy(col(\"avg_rating\").desc()).limit(5)\n",
    "\n",
    "# ğŸ’¡ ê²°ê³¼ ì¶œë ¥: movieId, title, genres, avg_rating\n",
    "top_recommendations.select(\"movieId\", \"title\", \"genres\", \"avg_rating\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "050c2914-2070-490d-b357-21d1c17ac722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          1. ë°ì´í„° ë¡œë”© (movies, ratings)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      2. ì¥ë¥´ í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ì/í† í°ìœ¼ë¡œ ë³€í™˜      â”‚\n",
    "â”‚      (Tokenizer â†’ CountVectorizer)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          3. TF-IDF ë²¡í„° ìƒì„± (ì¥ë¥´ â†’ ë²¡í„°)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    4. ì„ íƒí•œ ì˜í™”ì˜ TF-IDF ë²¡í„° ì¶”ì¶œ           â”‚\n",
    "â”‚    (target_vector)                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     5. ë‹¤ë¥¸ ì˜í™”ë“¤ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°       â”‚\n",
    "â”‚     (UDFë¡œ similarity ì»¬ëŸ¼ ìƒì„±)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       6. í‰ê·  í‰ì ê³¼ í•¨ê»˜ ì˜í™” ì •ë³´ ê²°í•©        â”‚\n",
    "â”‚       (title, genres, avg_rating, similarity)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         7. ìœ ì‚¬ë„ê°€ ë†’ì€ ìƒìœ„ 5ê°œ ì¶”ì²œ         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a76934be-90a6-4f59-b79c-d4f5397a27db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python #ì½”ì‹¸ì¸ ìœ ì‚¬ë„ë¥¼ í™œìš©í•œ ì˜í™” ì •ë³´ + ì¥ë¥´ ê¸°ë°˜ ìœ ì‚¬ ì˜í™” ì¶”ì²œ \n",
    "from pyspark.sql.functions import col, avg, lower, udf\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë”©\n",
    "df_movies = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/movies.csv\")   # movies\n",
    "df_ratings = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/ratings.csv\")  # ratings\n",
    "\n",
    "# 2. ì…ë ¥ ì˜í™” ID\n",
    "target_movie_id = \"14\"  # ì…ë ¥ê°’ì€ string í˜•ì‹\n",
    "\n",
    "# 3. ë°ì´í„° alias ì§€ì • ë° ì „ì²˜ë¦¬\n",
    "movies = df_movies.withColumn(\"genres\", lower(col(\"genres\"))).alias(\"movies\")\n",
    "ratings = df_ratings.withColumn(\"rating\", col(\"rating\").cast(\"float\")).alias(\"ratings\")\n",
    "\n",
    "# 4. TF-IDF: ì¥ë¥´ -> features\n",
    "tokenizer = Tokenizer(inputCol=\"genres\", outputCol=\"genre_words\")\n",
    "tokenized = tokenizer.transform(movies).alias(\"tokenized\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"genre_words\", outputCol=\"raw_features\")\n",
    "cv_model = cv.fit(tokenized)\n",
    "vectorized = cv_model.transform(tokenized).alias(\"vectorized\")\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(vectorized)\n",
    "tfidf = idf_model.transform(vectorized).alias(\"tfidf\")\n",
    "\n",
    "# 5. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "target_vector = tfidf.filter(col(\"movieId\") == target_movie_id).select(col(\"features\")).collect()[0][0]\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = float(v1.dot(v2))\n",
    "    norm_v1 = float(v1.norm(2))\n",
    "    norm_v2 = float(v2.norm(2))\n",
    "    return dot_product / (norm_v1 * norm_v2) if norm_v1 != 0 and norm_v2 != 0 else 0.0\n",
    "\n",
    "cosine_sim_udf = udf(lambda x: cosine_similarity(target_vector, x), DoubleType())\n",
    "tfidf = tfidf.withColumn(\"similarity\", cosine_sim_udf(col(\"features\")))\n",
    "\n",
    "# 6. í‰ê·  í‰ì  ê³„ì‚°\n",
    "avg_rating = ratings.groupBy(col(\"movieId\").alias(\"r_movieId\")) \\\n",
    "                    .agg(avg(\"rating\").alias(\"avg_rating\")) \\\n",
    "                    .alias(\"avg_rating\")\n",
    "\n",
    "# 7. ì˜í™” ì •ë³´ í†µí•©\n",
    "movies_with_info = tfidf \\\n",
    "    .join(\n",
    "        movies.select(\n",
    "            col(\"movieId\").alias(\"m_movieId\"),\n",
    "            col(\"title\").alias(\"movie_title\"),\n",
    "            col(\"genres\").alias(\"movie_genres\")\n",
    "        ),\n",
    "        col(\"movieId\") == col(\"m_movieId\")\n",
    "    ) \\\n",
    "    .join(\n",
    "        avg_rating,\n",
    "        col(\"movieId\") == col(\"r_movieId\"),\n",
    "        how=\"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"movieId\").alias(\"movie_id\"),\n",
    "        col(\"movie_title\"),\n",
    "        col(\"movie_genres\"),\n",
    "        col(\"avg_rating\"),\n",
    "        col(\"features\").alias(\"genre_features\"),\n",
    "        col(\"similarity\")\n",
    "    )\n",
    "\n",
    "# 8. ğŸ¬ ì„ íƒí•œ ì˜í™” ì •ë³´ ì¶œë ¥\n",
    "print(\"ğŸ¬ [ì„ íƒí•œ ì˜í™” ì •ë³´]\")\n",
    "movies_with_info.filter(col(\"movie_id\") == target_movie_id) \\\n",
    "    .select(\"movie_id\", \"movie_title\", \"movie_genres\", \"avg_rating\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# 9. ğŸ¯ ìœ ì‚¬ ì˜í™” ì¶”ì²œ ì¶œë ¥\n",
    "print(\"ğŸ¯ [ì¥ë¥´ ìœ ì‚¬ ì˜í™” ì¶”ì²œ]\")\n",
    "movies_with_info.filter(col(\"movie_id\") != target_movie_id) \\\n",
    "    .orderBy(col(\"similarity\").desc()) \\\n",
    "    .select(\"movie_id\", \"movie_title\", \"movie_genres\", \"avg_rating\", \"similarity\") \\\n",
    "    .limit(5) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "17d21a75-ed06-4471-9030-b63b32c5c640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window #í‰ê°€ì§€í‘œ MSE\n",
    "\n",
    "# ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ì˜í™”ë§Œ ì‚¬ìš© (ìê¸° ìì‹  ì œì™¸ë¨)\n",
    "similar_movies = movies_with_info.filter((col(\"movie_id\") != target_movie_id) & (col(\"similarity\") > 0))\n",
    "\n",
    "# ìœ ì‚¬ë„ í•© (ì •ê·œí™”ìš©)\n",
    "sim_sum = similar_movies.agg(F.sum(\"similarity\")).collect()[0][0]\n",
    "\n",
    "# ì˜ˆì¸¡ í‰ì  ê³„ì‚°: weighted avg\n",
    "predicted_rating = similar_movies.withColumn(\n",
    "    \"weighted_rating\", col(\"similarity\") * col(\"avg_rating\")\n",
    ").agg(\n",
    "    (F.sum(\"weighted_rating\") / F.lit(sim_sum)).alias(\"predicted_rating\")\n",
    ").collect()[0][0]\n",
    "\n",
    "# ì‹¤ì œ í‰ì  (ì˜í™” ì „ì²´ í‰ê· )\n",
    "actual_rating = movies_with_info.filter(col(\"movie_id\") == target_movie_id) \\\n",
    "    .select(\"avg_rating\").collect()[0][0]\n",
    "\n",
    "# MSE ê³„ì‚°\n",
    "mse = (actual_rating - predicted_rating) ** 2\n",
    "print(f\"ğŸ“Š Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefdd18e-9e71-461f-a236-ac65fb176d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##ALS ê¸°ë°˜ ì‚¬ìš©ì ì˜í™” ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da65b0c7-250d-4165-ad20-beae277c6dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "        [ì‚¬ìš©ì]\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "     â”‚  ì„¤ë¬¸ ì‘ë‹µ â”‚  â† ì‚¬ìš©ìì˜ ì·¨í–¥(ì¥ë¥´ ë“±)\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚ì‚¬ìš©ì ì„±í–¥ ë²¡í„° ìƒì„±â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "[ ì‚¬ìš©ì ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ ]  â† Cosine Similarity ë“±\n",
    "\n",
    "           â–²\n",
    "           â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "     â”‚ í‰ì  ë°ì´í„° â”‚  â† ratings.csv\n",
    "     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼\n",
    "   [ ALS ì¶”ì²œ ëª¨ë¸ ]  â† Spark ML ALS\n",
    "\n",
    "           â–¼\n",
    "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”‚ Hybrid ì¶”ì²œ ì¡°í•©â”‚ â† ì„¤ë¬¸ ê¸°ë°˜ + ALS ê¸°ë°˜ ê²°í•© (ê°€ì¤‘ í‰ê·  ë“±)\n",
    "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â–¼\n",
    "     [ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸]\n",
    "           â–¼\n",
    "       [ì‹œê°í™”/ì¶œë ¥]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a142bcf4-ae13-493e-acc3-362fc0a30c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col, explode, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "df_ratings = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "\n",
    "# 2. íƒ€ì… ë³€í™˜\n",
    "df_ratings = df_ratings.withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "\n",
    "df_movies = df_movies.withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "# 3. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë‚˜ëˆ„ê¸°\n",
    "(training, test) = df_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4. ALS ëª¨ë¸ ì •ì˜\n",
    "als = ALS(\n",
    "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\", nonnegative=True\n",
    ")\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµ\n",
    "model = als.fit(training)\n",
    "\n",
    "# 6. íŠ¹ì • ì‚¬ìš©ì ì¶”ì²œ (userId ì˜ˆ: 123)\n",
    "user_df = spark.createDataFrame([(123,)], [\"userId\"])\n",
    "userRecs = model.recommendForUserSubset(user_df, 10)\n",
    "\n",
    "# 7. ì¶”ì²œ ê²°ê³¼ ì •ë¦¬ (explode + index ë¶™ì´ê¸°)\n",
    "userRecsExploded = userRecs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                            .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "# ìˆœìœ„(index) ë¶€ì—¬\n",
    "windowSpec = Window.orderBy(col(\"rating\").desc())\n",
    "indexedRecs = userRecsExploded.withColumn(\"index\", row_number().over(windowSpec) - 1) \\\n",
    "                              .select(\"index\", \"movieId\")\n",
    "\n",
    "# 8. ê²°ê³¼ ì¶œë ¥\n",
    "display(indexedRecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5770d6f-9e29-4dd0-9bb4-26b7152a5218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ML Flow ì‚¬ìš© íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8297d73f-49fa-4fd3-b2d9-4ee2ea1c5ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# 11. MSE, RMSE í‰ê°€ì ì •ì˜\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",  # ë˜ëŠ” \"mse\"ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# 12. RMSE ê³„ì‚°\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "\n",
    "# 13. MSE ê³„ì‚° (ì˜µì…˜)\n",
    "evaluator_mse = RegressionEvaluator(\n",
    "    metricName=\"mse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "mse = evaluator_mse.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE) = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6ca01c-9778-4ced-b332-ddc55a96bc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 0. ì‹¤í—˜ ì„¤ì • (í•„ìˆ˜!)\n",
    "mlflow.set_experiment('/Users/1dt011@msacademy.msai.kr/1dt011')\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë”© ë° íƒ€ì… ë³€í™˜\n",
    "df_movies = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.movies\")\n",
    "df_ratings = spark.read.table(\"`1dt_team8_databricks`.`movielens-small`.ratings\")\n",
    "\n",
    "df_ratings = df_ratings.withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "df_movies = df_movies.withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "(training, test) = df_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 2. MLflow ì‹¤í—˜ ì‹œì‘\n",
    "with mlflow.start_run(run_name=\"ALS-Recommender\"):\n",
    "\n",
    "    # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "    rank = 10\n",
    "    maxIter = 10\n",
    "    regParam = 0.1\n",
    "\n",
    "    als = ALS(\n",
    "        userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "        rank=rank, maxIter=maxIter, regParam=regParam,\n",
    "        coldStartStrategy=\"drop\", nonnegative=True\n",
    "    )\n",
    "\n",
    "    model = als.fit(training)\n",
    "    predictions = model.transform(test)\n",
    "\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # ë¡œê¹…\n",
    "    mlflow.log_param(\"rank\", rank)\n",
    "    mlflow.log_param(\"maxIter\", maxIter)\n",
    "    mlflow.log_param(\"regParam\", regParam)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    mlflow.spark.log_model(model, \"als_model\")\n",
    "\n",
    "    print(f\"MLflow Run Completed - RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f8f46b1-30bf-402d-b83c-c436ef358a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 0. ì‹¤í—˜ ë“±ë¡\n",
    "mlflow.set_experiment(\"/Users/1dt011@msacademy.msai.kr/1dt011\")\n",
    "# mlflow.set_experiment(\"/Users/1dt011@msacademy.msai.kr/als-test\")\n",
    "\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³€í™˜\n",
    "def load_data():\n",
    "    df_movies = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/movies.csv\")\n",
    "    df_ratings = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/ratings.csv\")\n",
    "\n",
    "    df_movies = df_movies.withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "    df_ratings = df_ratings.withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "                           .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\")) \\\n",
    "                           .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "    \n",
    "    return df_ratings, df_movies\n",
    "\n",
    "# 2. íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n",
    "def train_pipeline(rank=10, maxIter=10, regParam=0.1):\n",
    "    df_ratings, df_movies = load_data()\n",
    "    train_data, test_data = df_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    als = ALS(\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        rank=rank,\n",
    "        maxIter=maxIter,\n",
    "        regParam=regParam,\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True\n",
    "    )\n",
    "\n",
    "    # ALSëŠ” Transformerì²˜ëŸ¼ ì‘ë™ (Pipelineì—ë„ ë„£ì„ ìˆ˜ ìˆìŒ)\n",
    "    pipeline = Pipeline(stages=[als])\n",
    "\n",
    "    # MLflow ì‹œì‘\n",
    "    with mlflow.start_run(run_name=\"ALS-Pipeline-Run\"):\n",
    "\n",
    "        model = pipeline.fit(train_data)\n",
    "\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    "        )\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "        # ë¡œê¹…\n",
    "        mlflow.log_param(\"rank\", rank)\n",
    "        mlflow.log_param(\"maxIter\", maxIter)\n",
    "        mlflow.log_param(\"regParam\", regParam)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.spark.log_model(model, \"als_pipeline_model\")\n",
    "\n",
    "        print(f\"[MLflow] Run logged with RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return model, df_movies\n",
    "\n",
    "# 3. ì‚¬ìš©ì ì¶”ì²œ í•¨ìˆ˜\n",
    "def recommend_for_user(model, user_id, df_movies, n=5):\n",
    "    user_df = spark.createDataFrame([(user_id,)], [\"userId\"])\n",
    "    recs = model.stages[0].recommendForUserSubset(user_df, n)\n",
    "\n",
    "    from pyspark.sql.functions import explode\n",
    "    recs = recs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "               .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\"))\n",
    "\n",
    "    final_recs = recs.join(df_movies, on=\"movieId\", how=\"left\")\n",
    "    return final_recs.select(\"userId\", \"title\", \"rating\")\n",
    "\n",
    "# 4. ì‹¤í–‰\n",
    "model, df_movies = train_pipeline()\n",
    "display(recommend_for_user(model, 123, df_movies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "635b6d15-48cb-4edf-9edc-5644c5af065f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. ìœ„ì ¯ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì…ë ¥\n",
    "dbutils.widgets.text(\"rank\", \"10\")\n",
    "dbutils.widgets.text(\"regParam\", \"0.1\")\n",
    "dbutils.widgets.text(\"maxIter\", \"10\")\n",
    "\n",
    "# 2. ì…ë ¥ê°’ ì¶”ì¶œ\n",
    "rank = int(dbutils.widgets.get(\"rank\"))\n",
    "regParam = float(dbutils.widgets.get(\"regParam\"))\n",
    "maxIter = int(dbutils.widgets.get(\"maxIter\"))\n",
    "\n",
    "# 3. ALS ëª¨ë¸ í•™ìŠµ\n",
    "als = ALS(\n",
    "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "    rank=rank, regParam=regParam, maxIter=maxIter,\n",
    "    coldStartStrategy=\"drop\", nonnegative=True\n",
    ")\n",
    "\n",
    "model = als.fit(training)\n",
    "\n",
    "# 4. í‰ê°€ ê²°ê³¼ ì €ì¥\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = model.transform(test)\n",
    "rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\").evaluate(predictions)\n",
    "\n",
    "# 5. ê²°ê³¼ í…Œì´ë¸” ì €ì¥\n",
    "from pyspark.sql import Row\n",
    "result_row = Row(rank=rank, regParam=regParam, maxIter=maxIter, rmse=rmse)\n",
    "result_df = spark.createDataFrame([result_row])\n",
    "result_df.write.mode(\"append\").saveAsTable(\"als_experiment_results\")\n",
    "\n",
    "# 6. ì¶”ì²œ ê²°ê³¼ ì €ì¥\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "userRecs.write.mode(\"overwrite\").saveAsTable(\"als_user_recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "036e8905-c429-4073-8e37-fc8e83b1b1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM als_experiment_results ORDER BY rmse ASC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "657b4d44-8411-44eb-b497-5a1ff15cff51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. ì…ë ¥ ìœ„ì ¯ (ë…¸íŠ¸ë¶ ìƒë‹¨ì—ì„œ íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥)\n",
    "dbutils.widgets.text(\"rank\", \"10\")\n",
    "dbutils.widgets.text(\"regParam\", \"0.1\")\n",
    "dbutils.widgets.text(\"maxIter\", \"10\")\n",
    "\n",
    "# 2. íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "rank = int(dbutils.widgets.get(\"rank\"))\n",
    "regParam = float(dbutils.widgets.get(\"regParam\"))\n",
    "maxIter = int(dbutils.widgets.get(\"maxIter\"))\n",
    "\n",
    "# 3. ë°ì´í„° ë¡œë”©\n",
    "df_ratings = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/ratings.csv\")\n",
    "df_movies = spark.read.option(\"header\", True).csv(\"/Volumes/1dt_team8/default/movies/movies.csv\")\n",
    "\n",
    "# 4. íƒ€ì… ë³€í™˜\n",
    "from pyspark.sql.functions import col\n",
    "df_ratings = df_ratings.withColumn(\"userId\", col(\"userId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"movieId\", col(\"movieId\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
    "df_movies = df_movies.withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
    "\n",
    "# 5. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "(training, test) = df_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 6. ALS ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(\n",
    "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "    rank=rank, regParam=regParam, maxIter=maxIter,\n",
    "    coldStartStrategy=\"drop\", nonnegative=True\n",
    ")\n",
    "model = als.fit(training)\n",
    "\n",
    "# 7. ì˜ˆì¸¡ ë° í‰ê°€\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# 8. ì‹¤í—˜ ê²°ê³¼ ì €ì¥\n",
    "from pyspark.sql import Row\n",
    "result = Row(rank=rank, regParam=regParam, maxIter=maxIter, rmse=rmse)\n",
    "result_df = spark.createDataFrame([result])\n",
    "result_df.write.mode(\"append\").saveAsTable(\"als_experiment_results\")\n",
    "\n",
    "# 9. ì¶”ì²œ ê²°ê³¼ ìƒì„± ë° ì €ì¥\n",
    "user_recs = model.recommendForAllUsers(5)  # ë˜ëŠ” íŠ¹ì • ìœ ì €ë§Œ ì¶”ì²œí•˜ë ¤ë©´ recommendForUserSubset()\n",
    "user_recs.write.mode(\"overwrite\").saveAsTable(\"als_user_recommendations\")\n",
    "\n",
    "# 10. (ì„ íƒ) ì¶”ì²œ ê²°ê³¼ ë³´ê¸° ì˜ˆì‹œ\n",
    "from pyspark.sql.functions import explode\n",
    "recs = user_recs.select(\"userId\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "                .select(\"userId\", col(\"rec.movieId\"), col(\"rec.rating\")) \\\n",
    "                .join(df_movies, on=\"movieId\", how=\"left\")\n",
    "\n",
    "display(recs.orderBy(\"userId\", \"rating\", ascending=[True, False]))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8846788788865742,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test (1)",
   "widgets": {
    "maxIter": {
     "currentValue": "10",
     "nuid": "e0daffc2-7f59-46a6-98a1-f5c3cb1b6560",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "maxIter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "maxIter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "rank": {
     "currentValue": "10",
     "nuid": "2493609f-1949-4523-bfb7-b0c0615f9c7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "rank",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": null,
      "name": "rank",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "regParam": {
     "currentValue": "0.1",
     "nuid": "81f5edf8-89fd-402e-92a4-001760157f8a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.1",
      "label": null,
      "name": "regParam",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.1",
      "label": null,
      "name": "regParam",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
