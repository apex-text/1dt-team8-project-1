{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a63e34d-3598-4d68-9e26-58c35dde5537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install optuna\n",
    "%pip install graphviz\n",
    "\n",
    "\n",
    "from graphviz import Digraph\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType, DoubleType, LongType, FloatType\n",
    "from mlflow.models.signature import infer_signature\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.xgboost\n",
    "from mlflow.models.signature import infer_signature\n",
    "import math\n",
    "from builtins import sum as py_sum\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shap\n",
    "import json\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa284b47-d962-47a9-b990-5bbfaeaff070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sparkXGBoost_final\") \\\n",
    "    .getOrCreate()\n",
    "mlflow.autolog(disable=True)\n",
    "mlflow.spark.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb46cb5-c0a1-4072-963f-a6f9974d1175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c20c36-c995-422e-9661-24d5b2075724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"1dt_team8_databricks\"\n",
    "schema = \"`final`\"\n",
    "path = f\"{catalog}.{schema}\"\n",
    "\n",
    "try:\n",
    "    train = spark.read.table(f\"{path}.train_temp\")\n",
    "    validation = spark.read.table(f\"{path}.validation_temp\")\n",
    "    test = spark.read.table(f\"{path}.test_temp\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from Unity Catalog Volume: {e}\")\n",
    "\n",
    "train = train.withColumn(\"label\", when(train[\"rating\"] >= 4, 1).otherwise(0))\n",
    "validation = validation.withColumn(\"label\", when(validation[\"rating\"] >= 4, 1).otherwise(0))\n",
    "test = test.withColumn(\"label\", when(test[\"rating\"] >= 4, 1).otherwise(0))\n",
    "\n",
    "train = train.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "validation = validation.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
    "test = test.withColumn(\"year\", col(\"year\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a39b18-7335-4eff-a3fa-0e6f4864f6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79705f7-ecb6-4673-9e1a-3c311f0d458e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_indexer = StringIndexer(inputCol=\"userId\", outputCol=\"userIndex\", handleInvalid=\"keep\")\n",
    "movie_indexer = StringIndexer(inputCol=\"movieId\", outputCol=\"movieIndex\", handleInvalid=\"keep\")\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"genres\", outputCol=\"genres_tokens\", pattern=\"\\\\|\")\n",
    "vectorizer = CountVectorizer(inputCol=\"genres_tokens\", outputCol=\"genres_vec\")\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=[\"genres_vec\", \"userIndex\", \"movieIndex\", \"year\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer, vectorizer, \n",
    "    user_indexer, movie_indexer, assembler_all\n",
    "])\n",
    "\n",
    "user_indexer_model = user_indexer.fit(train)\n",
    "pipeline_model = pipeline.fit(train)\n",
    "train_transformed = pipeline_model.transform(train)\n",
    "validation_transformed = pipeline_model.transform(validation)\n",
    "test_transformed = pipeline_model.transform(test)\n",
    "\n",
    "train_transformed.cache()\n",
    "validation_transformed.cache()\n",
    "test_transformed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bb8b1c-c799-49a4-ae42-8c2facf13b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i, stage in enumerate(pipeline_model.stages):\n",
    "    displayHTML(f\"<b>Stage {i}</b>: {stage.__class__.__name__}<br>Description: {stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe6508e-3f62-40ce-8b5f-1db17423b958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLFlow + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e3f06b-6c68-4f21-a296-665b4d597748",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "k =10\n",
    "mlflow.set_experiment(\"/1dt003MLflow\")\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3),\n",
    "        \"num_round\": trial.suggest_int(\"num_round\", 50, 100),\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "\n",
    "    model = SparkXGBClassifier(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"label\",\n",
    "        prediction_col=\"prediction\",\n",
    "        probability_col=\"probability\",\n",
    "        seed = 0,\n",
    "        **{\n",
    "            \"maxDepth\": param[\"max_depth\"],\n",
    "            \"eta\": param[\"eta\"],\n",
    "            \"numRound\": param[\"num_round\"],\n",
    "            \"evalMetric\": \"logloss\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_fitted = model.fit(train_transformed)\n",
    "    pred_test = model_fitted.transform(validation_transformed)\n",
    "    pred_test = pred_test.withColumn(\"y_pred_prob\", vector_to_array(col(\"probability\"))[1])\n",
    "\n",
    "    window_spec = Window.partitionBy(\"userIndex\").orderBy(col(\"y_pred_prob\").desc())\n",
    "    top_k = pred_test.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "                     .filter(col(\"rank\") <= k)\n",
    "\n",
    "    precision_df = top_k.groupBy(\"userIndex\").agg(avg(\"label\").alias(\"user_precision\"))\n",
    "    result = precision_df.select(avg(\"user_precision\").alias(\"precision_at_k\")).collect()\n",
    "\n",
    "    avg_precision_at_k = result[0][\"precision_at_k\"] if result and result[0][\"precision_at_k\"] is not None else 0.0\n",
    "\n",
    "    return -avg_precision_at_k\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "spark_params = {\n",
    "    \"max_depth\": best_params[\"max_depth\"],\n",
    "    \"eta\": best_params[\"eta\"],\n",
    "    \"num_round\": best_params[\"num_round\"],\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"features_col\": \"features\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"prediction_col\": \"prediction\",\n",
    "    \"probability_col\": \"probability\",\n",
    "    \"missing\": 0.0,\n",
    "    \"seed\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f7ba94-f77c-4eb7-9e5a-a51f779460ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainval_df = train_transformed.union(validation_transformed)\n",
    "\n",
    "final_model = SparkXGBClassifier(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"label\",\n",
    "    prediction_col=\"prediction\",\n",
    "    probability_col=\"probability\",\n",
    "    seed=0,\n",
    "    **{\n",
    "        \"maxDepth\": spark_params[\"max_depth\"],\n",
    "        \"eta\": spark_params[\"eta\"],\n",
    "        \"numRound\": spark_params[\"num_round\"],\n",
    "        \"evalMetric\": \"logloss\"\n",
    "    }\n",
    ")\n",
    "\n",
    "final_model_fitted = final_model.fit(trainval_df)\n",
    "\n",
    "test_pred = final_model_fitted.transform(test_transformed) \\\n",
    "    .withColumn(\"y_pred_proba\", vector_to_array(col(\"probability\"))[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dc56667-1cd2-43b1-b24f-0018e2516fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 사용자 별 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c7cf45-f322-4965-a361-46a4f53b63c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_users = test_transformed.select(\"userIndex\").distinct()\n",
    "\n",
    "seen_movies = (\n",
    "    train_transformed.select(\"userIndex\", \"movieIndex\")\n",
    "    .union(validation_transformed.select(\"userIndex\", \"movieIndex\"))\n",
    "    .join(test_users, on=\"userIndex\", how=\"inner\")\n",
    "    .distinct()\n",
    "    .groupBy(\"userIndex\")\n",
    "    .agg(collect_set(\"movieIndex\").alias(\"seen_movies\"))\n",
    ")\n",
    "\n",
    "pred_filtered = (\n",
    "    test_pred.join(seen_movies, on=\"userIndex\", how=\"left_outer\")\n",
    "    .filter((col(\"seen_movies\").isNull()) | (~array_contains(col(\"seen_movies\"), col(\"movieIndex\"))))\n",
    ")\n",
    "\n",
    "K = 10\n",
    "window_spec = Window.partitionBy(\"userIndex\").orderBy(col(\"y_pred_proba\").desc())\n",
    "\n",
    "top_k = (\n",
    "    pred_filtered.withColumn(\"rank\", row_number().over(window_spec))\n",
    "    .filter(col(\"rank\") <= K)\n",
    ")\n",
    "\n",
    "movie_index_map = (\n",
    "    train_transformed.select(\"movieIndex\", col(\"movieId\").alias(\"movieId_map\"))\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "movies_meta = (\n",
    "    train.select(\"movieId\", \"title\", \"genres\")\n",
    "    .dropDuplicates()\n",
    "    .withColumnRenamed(\"movieId\", \"movieId_meta\")\n",
    "    .withColumnRenamed(\"title\", \"title_meta\")\n",
    "    .withColumnRenamed(\"genres\", \"genres_meta\")\n",
    ")\n",
    "\n",
    "top_k_with_movieId = top_k.join(movie_index_map, on=\"movieIndex\", how=\"left\")\n",
    "\n",
    "top_k_with_meta = top_k_with_movieId.join(\n",
    "    movies_meta,\n",
    "    top_k_with_movieId[\"movieId_map\"] == movies_meta[\"movieId_meta\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "recommendations  = (\n",
    "    top_k_with_meta.select(\n",
    "        \"userIndex\",\n",
    "        col(\"title_meta\").alias(\"title\"),\n",
    "        col(\"y_pred_proba\").alias(\"predicted_rating\")\n",
    "    )\n",
    "    .orderBy(\"userIndex\", col(\"predicted_rating\").desc())\n",
    ")\n",
    "\n",
    "display(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e9dc64-25d1-407c-91ea-9f03b3d3357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 평가지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e1abbf-b3ae-4b35-a1c3-ced1a1d0bc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "# 1. 사용자별 Top-K 추천 항목 선택 (이미 만들어둔 top_k 활용)\n",
    "\n",
    "# 2. Precision@K 계산\n",
    "precision_df = top_k.groupBy(\"userIndex\") \\\n",
    "    .agg(\n",
    "        _sum(\"label\").alias(\"true_positives\"),\n",
    "        count(\"label\").alias(\"recommended_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"precision_at_k\", col(\"true_positives\") / col(\"recommended_count\"))\n",
    "\n",
    "mean_precision_at_k = precision_df.select(avg(\"precision_at_k\").alias(\"mean_precision_at_k\")) \\\n",
    "    .first()[\"mean_precision_at_k\"]\n",
    "\n",
    "print(f\"[Test Set] Precision@{K}: {mean_precision_at_k:.4f}\")\n",
    "\n",
    "# 3. 사용자별 실제 긍정 아이템 총 개수 (test_pred 기준)\n",
    "total_relevant_df = test_pred.groupBy(\"userIndex\") \\\n",
    "    .agg(_sum(\"label\").alias(\"total_relevant\"))\n",
    "\n",
    "# 4. 사용자별 Top-K 추천 내 실제 긍정 개수\n",
    "true_positives_df = top_k.groupBy(\"userIndex\") \\\n",
    "    .agg(_sum(\"label\").alias(\"true_positives\"))\n",
    "\n",
    "# 5. Recall@K 계산\n",
    "recall_df = true_positives_df.join(total_relevant_df, on=\"userIndex\") \\\n",
    "    .withColumn(\"recall_at_k\", col(\"true_positives\") / col(\"total_relevant\"))\n",
    "\n",
    "mean_recall_at_k = recall_df.select(avg(\"recall_at_k\").alias(\"mean_recall_at_k\")) \\\n",
    "    .first()[\"mean_recall_at_k\"]\n",
    "\n",
    "print(f\"[Test Set] Recall@{K}: {mean_recall_at_k:.4f}\")\n",
    "\n",
    "# 6. nDCG@K 계산을 위한 사용자별 label 리스트 수집 (top_k 기준, rank 정렬)\n",
    "user_labels_test = top_k.orderBy(\"userIndex\", \"rank\") \\\n",
    "    .groupBy(\"userIndex\") \\\n",
    "    .agg(collect_list(\"label\").alias(\"label_list\"))\n",
    "\n",
    "# DCG 계산 함수\n",
    "def dcg_at_k(labels, k):\n",
    "    if not labels:\n",
    "        return 0.0\n",
    "    return py_sum(rel / math.log2(idx + 2) for idx, rel in enumerate(labels[:k]))\n",
    "\n",
    "# IDCG 계산 함수\n",
    "def idcg_at_k(labels, k):\n",
    "    if not labels:\n",
    "        return 0.0\n",
    "    sorted_labels = sorted(labels, reverse=True)\n",
    "    return py_sum(rel / math.log2(idx + 2) for idx, rel in enumerate(sorted_labels[:k]))\n",
    "\n",
    "# UDF 등록\n",
    "dcg_udf = udf(lambda x: dcg_at_k(x, K), DoubleType())\n",
    "idcg_udf = udf(lambda x: idcg_at_k(x, K), DoubleType())\n",
    "\n",
    "# nDCG 계산\n",
    "ndcg_test_df = user_labels_test \\\n",
    "    .withColumn(\"dcg\", dcg_udf(\"label_list\")) \\\n",
    "    .withColumn(\"idcg\", idcg_udf(\"label_list\")) \\\n",
    "    .withColumn(\"ndcg_at_k\", col(\"dcg\") / col(\"idcg\"))\n",
    "\n",
    "mean_ndcg_at_k = ndcg_test_df.select(avg(\"ndcg_at_k\").alias(\"mean_ndcg_at_k\")) \\\n",
    "    .first()[\"mean_ndcg_at_k\"]\n",
    "\n",
    "print(f\"[Test Set] nDCG@{K}: {mean_ndcg_at_k:.4f}\")\n",
    "\n",
    "# 7. 시각화\n",
    "metrics = ['Precision@K', 'Recall@K', 'nDCG@K']\n",
    "scores = [mean_precision_at_k, mean_recall_at_k, mean_ndcg_at_k]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height + 0.01, f\"{height:.4f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.title(f'Evaluation Metrics @K={K}')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a69e39a6-e8b2-4f94-9fae-48483ed2c14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd37a0f-aacb-4e8d-b04e-e2f6863b3656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 예시\n",
    "best_params = {\n",
    "    \"maxDepth\": spark_params[\"max_depth\"],\n",
    "    \"eta\": spark_params[\"eta\"],\n",
    "    \"numRound\": spark_params[\"num_round\"],\n",
    "    \"evalMetric\": \"logloss\"\n",
    "}\n",
    "\n",
    "# 평가지표 예시\n",
    "metrics = {\n",
    "    \"precision_at_k\": mean_precision_at_k,\n",
    "    \"recall_at_k\": mean_recall_at_k,\n",
    "    \"ndcg_at_k\": mean_ndcg_at_k,\n",
    "    \"K\": K\n",
    "}\n",
    "\n",
    "# MLflow 실험명과 run 이름 설정\n",
    "mlflow.set_experiment(\"/1dt003MLflow\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost_Final_Model\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # 1. 하이퍼파라미터 기록\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # 2. 파이프라인 저장 (train 데이터로 fit된 전처리 파이프라인)\n",
    "    mlflow.spark.log_model(pipeline_model, artifact_path=\"preprocessing_pipeline\")\n",
    "    \n",
    "    # 3. 모델 저장 (train+val 데이터로 재학습한 최종 모델)\n",
    "    mlflow.spark.log_model(final_model_fitted, artifact_path=\"xgb_model\")\n",
    "    \n",
    "    # 4. 추천 결과 저장 (Spark DataFrame → Pandas → JSON)\n",
    "    recommendations_pd = recommendations.toPandas()\n",
    "    recommendations_json = recommendations_pd.to_json(orient=\"records\")\n",
    "    with tempfile.NamedTemporaryFile(\"w\", suffix=\".json\", delete=False) as f:\n",
    "        f.write(recommendations_json)\n",
    "        temp_recommendations_path = f.name\n",
    "    mlflow.log_artifact(temp_recommendations_path, artifact_path=\"recommendations\")\n",
    "    \n",
    "    # 5. 평가지표 기록\n",
    "    for key, value in metrics.items():\n",
    "        mlflow.log_metric(key, value)\n",
    "    \n",
    "    # 6. 하이퍼파라미터 별도 artifact로 저장 (선택사항)\n",
    "    with tempfile.NamedTemporaryFile(\"w\", suffix=\".json\", delete=False) as f:\n",
    "        json.dump(best_params, f)\n",
    "        temp_params_path = f.name\n",
    "    mlflow.log_artifact(temp_params_path, artifact_path=\"params\")\n",
    "\n",
    "print(f\"MLflow run completed. Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1e6207-a3ac-4042-8eec-9cfb5068709d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#신규 유저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f7128a-5072-4f4b-b8a2-d743ac503c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_set, lit, desc, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Step 1. 신규 유저가 본 영화 리스트\n",
    "new_user_seen_movies = [1, 10, 50, 300]\n",
    "new_user_set = set(new_user_seen_movies)\n",
    "\n",
    "# Step 2. 기존 유저별 시청 영화 집합 생성\n",
    "user_movie_sets = train.groupBy(\"userId\").agg(\n",
    "    collect_set(\"movieId\").alias(\"movie_set\")\n",
    ")\n",
    "\n",
    "# Step 3. Jaccard 유사도 계산 함수 정의\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1, set2 = set(set1), set(set2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return float(intersection) / union if union != 0 else 0.0\n",
    "\n",
    "jaccard_udf = udf(lambda x: jaccard_similarity(new_user_set, x), DoubleType())\n",
    "\n",
    "# Step 4. 가장 유사한 유저 1명 추출\n",
    "similar_users = user_movie_sets.withColumn(\"jaccard_sim\", jaccard_udf(col(\"movie_set\"))) \\\n",
    "                               .orderBy(desc(\"jaccard_sim\")) \\\n",
    "                               .limit(1)\n",
    "\n",
    "if similar_users.count() == 0:\n",
    "    print(\"❌ 유사한 유저를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    # Step 5. 유사 유저의 userId 및 userIndex 추출\n",
    "    top_user_id = similar_users.select(\"userId\").first()[\"userId\"]\n",
    "    top_user_index = user_indexer_model.transform(\n",
    "        spark.createDataFrame([(top_user_id,)], [\"userId\"])\n",
    "    ).select(\"userIndex\").first()[\"userIndex\"]\n",
    "\n",
    "    # Step 6. 신규 유저가 보지 않은 영화만 필터링 (train + test 기준)\n",
    "    all_movies = train.select(\"movieId\").union(test.select(\"movieId\")).dropDuplicates()\n",
    "    unseen_movies = all_movies.filter(~col(\"movieId\").isin(new_user_seen_movies))\n",
    "\n",
    "    # Step 7. 메타데이터 조인 (영화 제목 및 장르)\n",
    "    unseen_movies = unseen_movies.join(\n",
    "        movies_meta,\n",
    "        unseen_movies.movieId == movies_meta.movieId_meta,\n",
    "        how=\"left\"\n",
    "    ).select(\n",
    "        unseen_movies.movieId,\n",
    "        movies_meta[\"title_meta\"].alias(\"title\"),\n",
    "        movies_meta[\"genres_meta\"].alias(\"genres\")\n",
    "    )\n",
    "\n",
    "    # Step 8. 연도 정보 조인\n",
    "    movie_year_df = train.select(\"movieId\", \"year\").dropna().dropDuplicates([\"movieId\"])\n",
    "    unseen_movies = unseen_movies.join(movie_year_df, on=\"movieId\", how=\"left\")\n",
    "\n",
    "    # Step 9. userId 및 유사 유저 ID 덮어쓰기\n",
    "    unseen_movies = unseen_movies.withColumn(\"userId\", lit(top_user_id))\n",
    "\n",
    "    # Step 10. pipeline_model로 feature 추출\n",
    "    features_df = pipeline_model.transform(unseen_movies)\n",
    "\n",
    "    # Step 11. 유사 유저의 userIndex 덮어쓰기\n",
    "    features_df = features_df.withColumn(\"userIndex\", lit(top_user_index))\n",
    "\n",
    "    # Step 12. 중복 영화 제거\n",
    "    features_unique = features_df.dropDuplicates([\"movieId\"])\n",
    "\n",
    "    # Step 13. 예측 수행\n",
    "    predicted_df = final_model_fitted.transform(features_unique)\n",
    "\n",
    "    # Step 14. 긍정 클래스 확률 추출\n",
    "    predicted_df = predicted_df.withColumn(\"prediction_proba\", vector_to_array(col(\"probability\"))[1])\n",
    "\n",
    "    # Step 15. Top-K 추출\n",
    "    K = 10\n",
    "    top_k_df = predicted_df.orderBy(col(\"prediction_proba\").desc()).limit(K)\n",
    "\n",
    "    # Step 16. 결과 출력\n",
    "    top_k_df.select(\"movieId\", \"title\", \"prediction_proba\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8500c63-d5e3-48bd-8d6b-9cc858ab26db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 모델 서비스(배포 실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72cd3be-db35-4bb7-96b6-97e63dad7f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MovieRecommender(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # 파이프라인과 모델 로드\n",
    "        pipeline_uri = context.artifacts[\"pipeline_model_path\"]\n",
    "        self.pipeline_model = mlflow.spark.load_model(pipeline_uri)\n",
    "\n",
    "        model_uri = context.artifacts[\"xgboost_model_path\"]\n",
    "        self.final_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "        # 데이터 불러오기\n",
    "        catalog = \"1dt_team8_databricks\"\n",
    "        schema = \"final\"\n",
    "        path = f\"{catalog}.{schema}\"\n",
    "\n",
    "        self.train_df = self.spark.read.table(f\"{path}.train_temp\").cache()\n",
    "        self.movie_meta = self.train_df.select(\"movieId\", \"title\", \"genres\", \"year\").distinct().cache()\n",
    "        self.all_movies = self.movie_meta.select(\"movieId\").distinct().cache()\n",
    "        self.user_movies = (\n",
    "            self.train_df.groupBy(\"userId\")\n",
    "            .agg(F.collect_set(\"movieId\").alias(\"movie_set\"))\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        # UDF 등록 (load_context에서 한번만)\n",
    "        self.extract_prob_udf = udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "    def jaccard_similarity(self, set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    def find_most_similar_user(self, new_user_movies):\n",
    "        max_sim = 0\n",
    "        best_user = None\n",
    "        new_user_set = set(new_user_movies)\n",
    "        # 사용자 수가 많을 경우 성능 이슈 가능 → 실서비스는 별도 캐싱 권장\n",
    "        for row in self.user_movies.collect():\n",
    "            sim = self.jaccard_similarity(new_user_set, set(row[\"movie_set\"]))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                best_user = row[\"userId\"]\n",
    "        return best_user\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # 입력 검증\n",
    "        if \"userId\" not in model_input or \"seen_movie_ids\" not in model_input:\n",
    "            raise ValueError(\"Input must contain 'userId' and 'seen_movie_ids' columns\")\n",
    "\n",
    "        user_id = int(model_input[\"userId\"].iloc[0])\n",
    "        seen_movies = set(model_input[\"seen_movie_ids\"].iloc[0])\n",
    "\n",
    "        # 유저 존재 여부 체크\n",
    "        user_exists = self.user_movies.filter(col(\"userId\") == user_id).count() > 0\n",
    "\n",
    "        if user_exists:\n",
    "            exclude_movies = set(\n",
    "                self.user_movies.filter(col(\"userId\") == user_id).select(\"movie_set\").collect()[0][0]\n",
    "            ).union(seen_movies)\n",
    "        else:\n",
    "            similar_user = self.find_most_similar_user(seen_movies)\n",
    "            if similar_user is None:\n",
    "                exclude_movies = seen_movies\n",
    "            else:\n",
    "                similar_movies = self.user_movies.filter(col(\"userId\") == similar_user).select(\"movie_set\").collect()[0][0]\n",
    "                exclude_movies = set(similar_movies).union(seen_movies)\n",
    "\n",
    "        # 유저가 안본 영화 필터링\n",
    "        unseen_movies = self.all_movies.filter(~self.all_movies.movieId.isin(list(exclude_movies)))\n",
    "\n",
    "        user_unseen_df = unseen_movies.join(self.movie_meta, on=\"movieId\", how=\"left\") \\\n",
    "                                      .withColumn(\"userId\", lit(user_id))\n",
    "\n",
    "        # 파이프라인 변환\n",
    "        features_df = self.pipeline_model.transform(user_unseen_df)\n",
    "\n",
    "        # 예측 수행\n",
    "        predictions = self.final_model.transform(features_df)\n",
    "        predictions = predictions.withColumn(\"score\", self.extract_prob_udf(col(\"probability\")))\n",
    "\n",
    "        # 상위 10개 추천\n",
    "        top_k = predictions.orderBy(col(\"score\").desc()).limit(10)\n",
    "        top_k_with_title = top_k.select(\"movieId\", \"title\")\n",
    "\n",
    "        return top_k_with_title.toPandas()\n",
    "\n",
    "# 입력 예시 및 서명\n",
    "input_example = pd.DataFrame({\n",
    "    \"userId\": [1],\n",
    "    \"seen_movie_ids\": [[1, 2, 3]]\n",
    "})\n",
    "output_example = pd.DataFrame({\n",
    "    \"movieId\": [10],\n",
    "    \"title\": [\"Movie A\"]\n",
    "})\n",
    "\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Xgboost_Service\") as run:\n",
    "    # 1. 파이프라인 모델 저장\n",
    "    mlflow.spark.log_model(pipeline_model, artifact_path=\"pipeline_model\")\n",
    "\n",
    "    # 2. XGBoost Booster 모델 별도 저장 (final_model_fitted는 Booster 객체 또는 학습된 모델)\n",
    "    booster = final_model_fitted.get_booster()\n",
    "    local_path = \"/tmp/xgb_native.model\"\n",
    "    booster.save_model(local_path)\n",
    "    mlflow.xgboost.log_model(booster, artifact_path=\"xgboost_model\")\n",
    "\n",
    "    # 3. pyfunc 모델 저장\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"movie_recommender_Xgb\",\n",
    "        python_model=MovieRecommender(),\n",
    "        artifacts={\n",
    "            \"pipeline_model_path\": f\"runs:/{run.info.run_id}/pipeline_model\",\n",
    "            \"xgboost_model_path\": f\"runs:/{run.info.run_id}/xgboost_model\"\n",
    "        },\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    # 4. 모델 등록\n",
    "    model_uri = f\"runs:/{run.info.run_id}/movie_recommender_Xgb\"\n",
    "    mlflow.register_model(model_uri, \"movie_recommender_Xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bf2f6e-be45-472d-8994-5428a317826f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MovieRecommender(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # 파이프라인과 모델 로드\n",
    "        pipeline_uri = context.artifacts[\"pipeline_model_path\"]\n",
    "        self.pipeline_model = mlflow.spark.load_model(pipeline_uri)\n",
    "\n",
    "        model_uri = context.artifacts[\"xgboost_model_path\"]\n",
    "        self.final_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "        # 데이터 불러오기\n",
    "        catalog = \"1dt_team8_databricks\"\n",
    "        schema = \"final\"\n",
    "        path = f\"{catalog}.{schema}\"\n",
    "\n",
    "        self.train_df = self.spark.read.table(f\"{path}.train_temp\").cache()\n",
    "        self.movie_meta = self.train_df.select(\"movieId\", \"title\", \"genres\", \"year\").distinct().cache()\n",
    "        self.all_movies = self.movie_meta.select(\"movieId\").distinct().cache()\n",
    "        self.user_movies = (\n",
    "            self.train_df.groupBy(\"userId\")\n",
    "            .agg(F.collect_set(\"movieId\").alias(\"movie_set\"))\n",
    "            .cache()\n",
    "        )\n",
    "\n",
    "        # UDF 등록 (load_context에서 한번만)\n",
    "        self.extract_prob_udf = udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "    def jaccard_similarity(self, set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    def find_most_similar_user(self, new_user_movies):\n",
    "        max_sim = 0\n",
    "        best_user = None\n",
    "        new_user_set = set(new_user_movies)\n",
    "        # 사용자 수가 많을 경우 성능 이슈 가능 → 실서비스는 별도 캐싱 권장\n",
    "        for row in self.user_movies.collect():\n",
    "            sim = self.jaccard_similarity(new_user_set, set(row[\"movie_set\"]))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                best_user = row[\"userId\"]\n",
    "        return best_user\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # 입력 검증\n",
    "        if \"userId\" not in model_input or \"seen_movie_ids\" not in model_input:\n",
    "            raise ValueError(\"Input must contain 'userId' and 'seen_movie_ids' columns\")\n",
    "\n",
    "        user_id = int(model_input[\"userId\"].iloc[0])\n",
    "        seen_movies = set(model_input[\"seen_movie_ids\"].iloc[0])\n",
    "\n",
    "        # 유저 존재 여부 체크\n",
    "        user_exists = self.user_movies.filter(col(\"userId\") == user_id).count() > 0\n",
    "\n",
    "        if user_exists:\n",
    "            exclude_movies = set(\n",
    "                self.user_movies.filter(col(\"userId\") == user_id).select(\"movie_set\").collect()[0][0]\n",
    "            ).union(seen_movies)\n",
    "        else:\n",
    "            similar_user = self.find_most_similar_user(seen_movies)\n",
    "            if similar_user is None:\n",
    "                exclude_movies = seen_movies\n",
    "            else:\n",
    "                similar_movies = self.user_movies.filter(col(\"userId\") == similar_user).select(\"movie_set\").collect()[0][0]\n",
    "                exclude_movies = set(similar_movies).union(seen_movies)\n",
    "\n",
    "        # 유저가 안본 영화 필터링\n",
    "        unseen_movies = self.all_movies.filter(~self.all_movies.movieId.isin(list(exclude_movies)))\n",
    "\n",
    "        user_unseen_df = unseen_movies.join(self.movie_meta, on=\"movieId\", how=\"left\") \\\n",
    "                                      .withColumn(\"userId\", lit(user_id))\n",
    "\n",
    "        # 파이프라인 변환\n",
    "        features_df = self.pipeline_model.transform(user_unseen_df)\n",
    "\n",
    "        # 예측 수행\n",
    "        predictions = self.final_model.transform(features_df)\n",
    "        predictions = predictions.withColumn(\"score\", self.extract_prob_udf(col(\"probability\")))\n",
    "\n",
    "        # 상위 10개 추천\n",
    "        top_k = predictions.orderBy(col(\"score\").desc()).limit(10)\n",
    "        top_k_with_title = top_k.select(\"movieId\", \"title\")\n",
    "\n",
    "        return top_k_with_title.toPandas()\n",
    "\n",
    "# 입력 예시 및 서명\n",
    "input_example = pd.DataFrame({\n",
    "    \"userId\": [1],\n",
    "    \"seen_movie_ids\": [[1, 2, 3]]\n",
    "})\n",
    "output_example = pd.DataFrame({\n",
    "    \"movieId\": [10],\n",
    "    \"title\": [\"Movie A\"]\n",
    "})\n",
    "\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Xgboost_Service\") as run:\n",
    "    # 1. 파이프라인 모델 로컬 저장\n",
    "    pipeline_local_path = \"/tmp/pipeline_model\"\n",
    "    mlflow.spark.save_model(pipeline_model, pipeline_local_path)\n",
    "    mlflow.spark.log_model(pipeline_model, artifact_path=\"pipeline_model\")\n",
    "\n",
    "    # 2. XGBoost Booster 모델 로컬 저장\n",
    "    booster = final_model_fitted.get_booster()\n",
    "    xgb_local_path = \"/tmp/xgb_model\"\n",
    "    os.makedirs(xgb_local_path, exist_ok=True)\n",
    "    booster.save_model(f\"{xgb_local_path}/model.bst\")\n",
    "    mlflow.xgboost.log_model(booster, artifact_path=\"xgboost_model\")\n",
    "\n",
    "    # 3. pyfunc 모델 저장 시 artifacts에 로컬 경로 지정\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"movie_recommender_Xgboost\",\n",
    "        python_model=MovieRecommender(),\n",
    "        artifacts={\n",
    "            \"pipeline_model_path\": pipeline_local_path,  # 로컬 경로 지정\n",
    "            \"xgboost_model_path\": xgb_local_path         # 로컬 경로 지정\n",
    "        },\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    # 4. 모델 등록\n",
    "    model_uri = f\"runs:/{run.info.run_id}/movie_recommender_Xgboost\"\n",
    "    mlflow.register_model(model_uri, \"movie_recommender_Xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53411923-827f-4e98-9620-15acd7d46de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 전체 유저 featureImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b90bad-939d-4558-a872-5989f809fa79",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1749488446153}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyAxLiBmZWF0dXJlX25hbWVzIOyEpOyglQp2ZWN0b3JpemVyX21vZGVsID0gTm9uZQpmb3Igc3RhZ2UgaW4gcGlwZWxpbmVfbW9kZWwuc3RhZ2VzOgogICAgaWYgc3RhZ2UudWlkLnN0YXJ0c3dpdGgoIkNvdW50VmVjdG9yaXplciIpOgogICAgICAgIHZlY3Rvcml6ZXJfbW9kZWwgPSBzdGFnZQogICAgICAgIGJyZWFrCgpnZW5yZV92b2NhYiA9IHZlY3Rvcml6ZXJfbW9kZWwudm9jYWJ1bGFyeSAgCmdlbnJlX2ZlYXR1cmVfbmFtZXMgPSBbImdlbnJlXyIgKyBnZW5yZSBmb3IgZ2VucmUgaW4gZ2VucmVfdm9jYWJdCmZlYXR1cmVfbmFtZXMgPSBnZW5yZV9mZWF0dXJlX25hbWVzICsgWyJ1c2VySW5kZXgiLCAibW92aWVJbmRleCIsICJ5ZWFyIl0KCiMgMi4gdGVzdF90cmFuc2Zvcm1lZOyXkOyEnCBmZWF0dXJlcyDstpTstpwgKOyghOyytCBvciDsnbzrtoApCnNhbXBsZWRfZmVhdHVyZXMgPSB0ZXN0X3RyYW5zZm9ybWVkLnNlbGVjdCgiZmVhdHVyZXMiKS5saW1pdCg1MDApLmNvbGxlY3QoKQpYID0gbnAudnN0YWNrKFtyb3dbImZlYXR1cmVzIl0udG9BcnJheSgpIGZvciByb3cgaW4gc2FtcGxlZF9mZWF0dXJlc10pICAjICg1MDAsIG5fZmVhdHVyZXMpCgojIDMuIFhHQm9vc3QgQm9vc3RlciDstpTstpwg67CPIFNIQVAg6rOE7IKwCnhnYl9zdGFnZSA9IGZpbmFsX21vZGVsLnN0YWdlc1stMV0KYm9vc3RlciA9IHhnYl9zdGFnZS5nZXRfYm9vc3RlcigpCgpleHBsYWluZXIgPSBzaGFwLlRyZWVFeHBsYWluZXIoYm9vc3RlcikKc2hhcF92YWx1ZXMgPSBleHBsYWluZXIuc2hhcF92YWx1ZXMoWCkKCnN1bW1hcnlfZGYgPSBwZC5EYXRhRnJhbWUoewogICAgJ0ZlYXR1cmUnOiBmZWF0dXJlX25hbWVzLAogICAgJ01lYW5fQWJzX1NIQVAnOiBucC5hYnMoc2hhcF92YWx1ZXMpLm1lYW4oYXhpcz0wKQp9KS5zb3J0X3ZhbHVlcyhieT0nTWVhbl9BYnNfU0hBUCcsIGFzY2VuZGluZz1GYWxzZSkKCmRpc3BsYXkoc3VtbWFyeV9kZikgICMgRGF0YWJyaWNrc+yXkOyEnCBCYXIgY2hhcnTroZwg67O06riwIOy2lOyynA==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewbf65030\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewbf65030\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewbf65030\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewbf65030) SELECT `Feature`,`Mean_Abs_SHAP` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewbf65030\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Feature Importance",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Feature",
             "id": "column_dc4d7694307"
            },
            "y": [
             {
              "column": "Mean_Abs_SHAP",
              "id": "column_dc4d7694304"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": false,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": ""
           },
           "seriesOptions": {
            "Mean_Abs_SHAP": {
             "color": "#077A9D",
             "name": "",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": false,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": null
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "중요도"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "db838fb7-71be-4782-b2e3-321b5fb857e6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 43.375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "Feature",
           "type": "column"
          },
          {
           "column": "Mean_Abs_SHAP",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer_model = None\n",
    "for stage in pipeline_model.stages:\n",
    "    if stage.uid.startswith(\"CountVectorizer\"):\n",
    "        vectorizer_model = stage\n",
    "        break\n",
    "\n",
    "genre_vocab = vectorizer_model.vocabulary  \n",
    "genre_feature_names = [\"genre_\" + genre for genre in genre_vocab]\n",
    "feature_names = genre_feature_names + [\"userIndex\", \"movieIndex\", \"year\"]\n",
    "\n",
    "sampled_features = test_transformed.select(\"features\").limit(500).collect()\n",
    "X = np.vstack([row[\"features\"].toArray() for row in sampled_features])\n",
    "\n",
    "booster = final_model_fitted.get_booster()\n",
    "\n",
    "explainer = shap.TreeExplainer(booster)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Mean_Abs_SHAP': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by='Mean_Abs_SHAP', ascending=False)\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eef125a-8a17-4aa6-917a-ad8a4fd2ce77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 특정 유저에게 왜 해당 영화 추천했는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6b6204-2d42-4155-99ca-b4d580f0db94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. CountVectorizer에서 genre feature 이름 추출\n",
    "vectorizer_model = next(\n",
    "    (stage for stage in pipeline_model.stages if stage.uid.startswith(\"CountVectorizer\")),\n",
    "    None\n",
    ")\n",
    "if vectorizer_model is None:\n",
    "    raise ValueError(\"CountVectorizer가 pipeline_model에 존재하지 않습니다.\")\n",
    "\n",
    "genre_vocab = vectorizer_model.vocabulary\n",
    "genre_feature_names = [\"genre_\" + genre for genre in genre_vocab]\n",
    "feature_names = genre_feature_names + [\"userIndex\", \"movieIndex\", \"year\"]\n",
    "\n",
    "# 2. 대상 사용자 선택\n",
    "target_user = 1\n",
    "\n",
    "# 3. 추천 영화 1개 선택\n",
    "user_recs = recommendations.filter(col(\"userIndex\") == target_user).limit(1).collect()\n",
    "if not user_recs:\n",
    "    print(f\"User {target_user} has no recommendations.\")\n",
    "else:\n",
    "    rec_row = user_recs[0]\n",
    "    title = rec_row[\"title\"]\n",
    "\n",
    "    # movieId 추출 (title 기반)\n",
    "    movie_row = movies_meta.filter(col(\"title_meta\") == title).select(\"movieId_meta\").collect()\n",
    "    if not movie_row:\n",
    "        print(f\"title에 해당하는 movieId를 찾을 수 없습니다: {title}\")\n",
    "    else:\n",
    "        movie_id = movie_row[0][\"movieId_meta\"]\n",
    "\n",
    "        # test_transformed에서 features 벡터 추출\n",
    "        feature_row = test_transformed.filter(\n",
    "            (col(\"userIndex\") == target_user) & (col(\"movieId\") == movie_id)\n",
    "        ).select(\"features\").collect()\n",
    "\n",
    "        if not feature_row:\n",
    "            print(f\"feature vector를 찾을 수 없습니다: userIndex={target_user}, movieId={movie_id}\")\n",
    "        else:\n",
    "            feature_vector = feature_row[0][\"features\"].toArray().reshape(1, -1)\n",
    "\n",
    "            # final_model은 SparkXGBClassifierModel -> booster 추출\n",
    "            booster = final_model_fitted.get_booster()  # SparkXGBClassifierModel 기준\n",
    "\n",
    "            explainer = shap.TreeExplainer(booster)\n",
    "            shap_values = explainer.shap_values(feature_vector)\n",
    "\n",
    "            print(f\"\\n🎬 Movie Title: {title}\")\n",
    "            print(f\"🎥 MovieId: {movie_id}\")\n",
    "            genres_row = movies_meta.filter(col(\"movieId_meta\") == movie_id).select(\"genres_meta\").collect()\n",
    "            genres = genres_row[0][\"genres_meta\"] if genres_row else \"Unknown\"\n",
    "            print(f\"📚 Genres: {genres}\")\n",
    "\n",
    "            display(shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b0877b-d197-414a-9f9b-dec5d040f640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 예: SHAP 값, 피처 이름, 실제 feature 값이 있다고 가정\n",
    "shap_vals = shap_values[0]  # 1개 샘플에 대한 SHAP 값\n",
    "feature_vals = feature_vector[0]  # 실제 피처 값\n",
    "names = feature_names\n",
    "\n",
    "# SHAP 값 절대값 기준으로 Top-N 영향력 높은 피처만 보기\n",
    "top_n = 10\n",
    "indices = np.argsort(np.abs(shap_vals))[-top_n:][::-1]\n",
    "\n",
    "top_names = [names[i] for i in indices]\n",
    "top_shap_vals = [shap_vals[i] for i in indices]\n",
    "top_feature_vals = [feature_vals[i] for i in indices]\n",
    "\n",
    "# 수치형 막대 그래프\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(top_names, top_shap_vals, color=[\"#ff6f69\" if v < 0 else \"#88d8b0\" for v in top_shap_vals])\n",
    "plt.xlabel(\"SHAP Value (Feature Impact)\")\n",
    "plt.title(f\"Top {top_n} SHAP Feature Importances\")\n",
    "\n",
    "# 실제 피처 값 같이 표시\n",
    "for i, (bar, val) in enumerate(zip(bars, top_feature_vals)):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\" = {val:.2f}\", va='center')\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adc5ebee-a23a-407d-91b1-a308f8309223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b9b5d7-9e85-4bd1-b774-4c121bc0573c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_id = \"67ff5e58e5dc47d2a114e83c36d61f06\"\n",
    "model_uri = f\"runs:/{run_id}/xgb_model\"\n",
    "\n",
    "preprocessing_uri = f\"runs:/{run_id}/preprocessing_pipeline\"\n",
    "pipeline_model = mlflow.spark.load_model(preprocessing_uri)\n",
    "\n",
    "final_model_fitted = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "test_transformed = pipeline_model.transform(test)\n",
    "test_df = final_model_fitted.transform(test_transformed)\n",
    "user_indexer_model = pipeline_model.stages[2]\n",
    "\n",
    "test_pred = final_model_fitted.transform(test_df) \\\n",
    "    .withColumn(\"y_pred_proba\", vector_to_array(col(\"probability\"))[1])\n",
    "\n",
    "# artifact URI 구성\n",
    "artifact_uri = f\"runs:/{run_id}/recommendations/tmpq44uxkn1.json\"\n",
    "\n",
    "# 추천 결과 다운로드 (로컬 MLflow 서버 또는 Databricks 환경 따라 다름)\n",
    "local_path = mlflow.artifacts.download_artifacts(artifact_uri)\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(local_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    recommendations = json.load(f)\n",
    "\n",
    "recommendations = [\n",
    "    [int(item[\"userIndex\"]), item[\"title\"], item[\"predicted_rating\"]]\n",
    "    for item in recommendations\n",
    "]\n",
    "movies_meta = (\n",
    "    train.select(\"movieId\", \"title\", \"genres\")\n",
    "    .dropDuplicates()\n",
    "    .withColumnRenamed(\"movieId\", \"movieId_meta\")\n",
    "    .withColumnRenamed(\"title\", \"title_meta\")\n",
    "    .withColumnRenamed(\"genres\", \"genres_meta\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5428492087700176,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "XGBoost",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
